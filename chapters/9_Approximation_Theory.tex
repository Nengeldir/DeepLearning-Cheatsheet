\section{Approximation Theory}

\subsection{Compositions of Maps}

\Def[Linear Function] A function \(f\colon\R^n\to\R^m\) is a linear function if the following properties hold
\begin{itemize}
  \item \(\displaystyle\forall\vx,\vx'\in\R^n\colon f(\vx + \vx') = f(\vx) + f(\vx')\)
  \item \(\displaystyle\forall\vx\in\R\,\forall\alpha\in\R\colon f(\alpha\vx) = \alpha f(\vx)\)
\end{itemize}

\sep

\Thm \(f\colon\R^n\to\R\text{ is linear }\Longleftrightarrow f(\vx) = \vw^\T\vx\) for some \(\vw\in\R^n\)

\sep

\Def[Hyperplane]

\(H: =\dset{\vx}{\scprod{\vw,\vx - \vp} = 0} = \dset{\vx}{\scprod{\vw,\vx} = b}\)

where \(b = \scprod{\vw,\vp}\). \(\vw = \)normal vector, \(\vp\) points onto a point on the plane.

\sep

\Def[Level Sets] of a function \(f\colon\R^n\to\R\) is a one-parametric family of sets defined as

\[L_f(c): =\dset{\vx}{f(\vx) = c} = f^{-1}(c)\subseteq\R^n.\]

\sep

\Thm[Comp. of Lin. Maps/- is a Lin. Map/Unit]

Let \(F_1,\ldots,F_L\) be linear maps, then \(F = F_L\circ \cdots\circ F_2\circ F_1\) is also a linear map.

\ssep

\Cor Every \(L\)-layer NN of linear layer collapses to a 1-layer NN. Further note that hereby

\( \rank(F)\equiv\dim(\im(F))\leq\min_{i\in\set{1,\ldots,L}}\rank(F_i).\)

\subsection{Universal Approximation with Ridge Functions}

\Def[Ridge Function] \(f\colon\R^n\to\R\) is a \emph{ridge function}, if it can be written as \(f(\vx) = \sigma(\vw^\T\vx + b)\) for some \(\sigma\colon\R\to\R\), \(\vw\in\R^n\), \(b\in\R\).

\begin{itemize}
  \item \(\overline{f}(\vx) = \vw^\T\vx + b\) (linear part)
  \item \(L_f(c) =  \bigcup_{d\in\sigma^{-1}(c)}L_{\overline{f}}(d) =  \bigcup_{d\in L_{\sigma}(c)}L_{\overline{f}}(d)\)
  \item if \(\sigma\) is differentiable at \(z = \vw^\T\vx + b\) then\\
  \(\nabla_{\vx} f(\vx) =  \sigma'(z)\nabla_{\vx}\overline{f}(\vx) = \sigma'(z)\vw = \sigma'(\vw^\T\vx + b)\vw\)
  \item a ridge function picks out one direction of change (linear part), and then models the rate of change in that chosen direction via \(\sigma\)
\end{itemize}

\sep

\Thm Let \(f\colon\R^n\to\R\) be differentiable at \(\vx\). Then either \(\nabla_{\vx}f(\vx) = 0\), or  \(\nabla_{\vx}f(\vx)\perp L_f(f(\vx))\)

\todo{What's the difference between orthogonality and \(=0\)???}

\sep

\Def[Universe of Ridge Functions for some \(\sigma\colon\R\to\R\)]\\

\(\cG_{\sigma}^n : = \dset{g}{g(\vx) = \sigma(\vw^\T\vx + b),\, \vw\in\R^n,\, b\in\R,\, \sigma\colon\R\to\R}\)

\sep

\Def[Universe of Continuous Ridge Functions]

\(\cG^n: =\bigcup_{\sigma\in C(\R)}\cG_{\sigma}^n.\)

\sep

\Thm Composition of continuous functions is continuous.

\Thm (Hence) \(\cG^n\subseteq C(\R^n)\).

\sep

\Def[Span of Universe of Continuous Ridge Functions]

\(\cH^n: =\spn(\cG^n) =  \dset{h}{h = \sum_{j = 1}^r g_j,\, g_j\in\cG^n}.\)

\sep

\Def[Dense Function Class \(\cH\) in \(C(\R^d)\)] 

A \emph{function class} \(\cH\subseteq C(\R^d)\) is \emph{dense} in \(C(\R^d)\), iff

\[\forall f\in C(\R^d)\quad\forall\epsilon>0 \quad\forall K\subset\R^d, \, K\text{ compact}\]

\[\exists h\in\cH\colon\max_{x\in K}\abs{f(\vx) - h(\vx)} = \norm{f - h}_{\infty,K}<\epsilon.\]

\sep

\Thm[Density of Span of Continuous Ridge Functions]

\(\cH^n\) is \emph{dense} in \(C(\R^n)\). Note: we can absorb the linear combination weights within the functions \(g_j\).

\sep

So, we can approximate any \(f\in C(\R^n)\) through linear combinations of members in \(\cG^n\). This gives rise to the idea of building a 1-layer network with adaptive ridge functions (rather impractical). In the following we'll see how we can limit ourselves to one specific \(\sigma\) s.t. \(\spn(\cG_\sigma^n)\) is still dence in \(C(\R^n)\). And we'll see how we can simplify the discussion through dimension lifting.

\sep

\Thm[Approximation Theorem, 1993]

\(\sigma\in C^\infty(\R)\), \(\sigma\) not polynomial \(\Longrightarrow\) \(\cH_\sigma^1\) is dense in \(C(\R)\).

\sep

\Cor MLPs with one hidden layer, and any non-polynomial, \emph{smooth} activation function are a universal function approximators.

\Com we don't know how many hidden units are needed; the requirement that \(f\) is smooth can be substantially weakened (see results with ReLUs).

\sep

\Lem MLPs with one hidden layer and a polynomial activation function are \emph{not} universal function approximators.

\sep

\Lem[Dimension Lifting]

\( \cH_{\sigma}^1\text{ \emph{dense} in }C(\R)\Longrightarrow\forall n\geq 1\colon\cH_{\sigma}^n\text{ \emph{dense} in }C(\R^n)\)

\Com So we can lift the \emph{density property of ridge functions} from \(C(\R)\) to \(C(\R^n)\).

\sep

\textbf{Summary}
\begin{align*}
    \begin{matrix}
        \sigma\in C^\infty(\R),\\
        \sigma \text{ not polynomial}
    \end{matrix} 
    \stackrel{\text{Approx. Thm.}}{\Longrightarrow} \cH_\sigma^1 \text{ is dense in } C(\R) \\
    \stackrel{\text{Dim. Lifting}}{\Longrightarrow} \forall n\colon \cH_\sigma^n \text{ is dense in } C(\R^n)
\end{align*}

\subsection{Rectification Networks}

\sep

\Thm Any \(f\in C([0;1])\) can be uniformly approximated to arbitrary precision by a polygonal line (c.f. Shektman, 1982). Or in other words:

\(\cH = \set{\text{p.w. linear functions}}\) is \emph{dense} in \(C([0,1])\).

\sep

\Thm Lebesgue showed how a polygonal line with \(m\) pieces can be written 

\(\displaystyle g(x) = ax + b + \sum_{i = 1}^{m - 1} c_k(x - x_i)_{+}\) (ReLU function approx.)

\begin{itemize}
  \item Knots: \(0 = x_0<x_1<\cdots <x_{m - 1}<x_{m} = 1\)
  \item \(m + 1\) parameters: \(a,b,c_1,\ldots,c_{m - 1}\)
\end{itemize}

\todo{Write down how to compute the parameters}.

With the dimension lifting theorem we can lift this property from 1D to \(n\)D.

\ssep

Note that there's an alternative representation of the above through AVUs

\(g(x) = a'x + b' + \sum_{i = 1}^m - 1 c_i'\abs{x - x_i}\)

\todo{Write down how to compute the parameters}.

\sep

\Thm Networks with one hidden layer of ReLUs are universal function approximators.

\Com We can thus use a restricted set of activation functions (ReLUs). But still we don't know how many hidden units we need.

\subsubsection{Piecewise Linear Functions and Half Spaces}

So the ReLU and the AVU define a \emph{piecewise linear function} with 2 pieces. Hereby, \(\R^n\) is partitioned into two open half spaces (and a border face):

\begin{itemize}
  \item \(H^+: =\dset{\vx}{\vw^\T\vx + b>0}\subset\R^n\)
  \item \(H^-: =\dset{\vx}{\vw^\T\vx + b<0}\subset\R^n\)
  \item \(H^0: =\dset{\vx}{\vw^\T\vx + b = 0} = \R^n\setminus H^+\setminus H^-\subset\R^n\)
\end{itemize}

Further note that

\begin{itemize}
  \item \(g_{(\argdot)_+}(H^0) = g_{\abs{\argdot}}(H^0) = 0\)
  \item \(g_{(\argdot)_+}(H^{-}) = 0\)
  \item \(g_{\abs{\argdot}}(\vx) = g_{\abs{\argdot}}(\vv - \vx)\) with \(\vv = -2b\frac{\vw}{\norm{\vw}_2^2}\) (mirroring at \(\vw\): equivalent to subtracting the projection of \(\vx\) onto \(\vw\) twice from \(\vx\))
\end{itemize}

\sep

Partitions of ReLUs go to infinity, even if we have no examples there \(\to\) weak to extrapolation errors (adversarial examples). However, if you have enough data, then you can overcome this, because there won't be any new examples that lie outside of the training data regions.

\subsubsection{Linear Combinations of ReLUs}

\Thm[Zaslavsky, 1975] By linearly combining \(m\) rectified units \(\R^n\) can be partitioned at most into \(R(m)\) cells: 

\[R(m)\leq\sum_{i = 0}^{\min\set{m,n}}\binom{m}{i}\]

\Cor If classes \(m\leq n\) we have \(R(m) = 2^m\) (exponential growth).

\Cor For any input size \(n\) we have \(R(m)\in\BigO(m^n)\) (polynomial slow-down in number of cells, limited by the input space dimension).

\subsubsection{Deep Combination of ReLUs}

\Thm[Montufar et al, 2014] If we process \(n\)-dim. inputs through \(L\) ReLU layers with widths \(m_1,\ldots,m_L\in\BigO(m)\).Then \(\R^n\) can be partitioned into at most \(R(m,L)\) layers:

\[R(m,L)\in\BigO\left(\left(\frac{m}{n}\right)^{n(L - 1)}m^n\right)\]

\subsubsection{Hinging Hyperplanes}

\Def[Hinge Function (extension of ReLU)]

If \(g\colon\R^n\to\R\) can be written with parameters \(\vw_1,\vw_2\in\R^n\) and \(b_1,b_2\in\R\) as below it is called a hinge function

\[g(\vx) = \max\left(\vw_1^\T\vx + b_1,\vw_2^\T\vx + b_2\right)\]

\begin{itemize}
  \item two hyperplanes, ``glued'' together at their intersection. So for the intersection it holds that: \(\vw_1^\T\vx + b_1 = \vw_2^\T\vx + b_2\).
  \item Representational power: \(2\max(f,g) = f + g + \abs{f - g}.\)
  \todo{Didnt' understand that}
\end{itemize}

The good thing is that these hyperplanes (as opposed to the ReLU) don't interact only in one dimension (\(\vw\)), but they interact in two dimensions \(\vw_1,\vw_2\).

\sep

\Def[\(k\)-Hinge Function] \(g(\vx) = \max(\vw_1^\T\vx + b_1,\ldots,\vw_k^\T\vx + b_k)\).

\sep

\Thm[Wang and Sun, 2005] Every continuous p.w. linear function from \(\R^n\to\R\) can be written as a signed sum of \(k\)-Hinges with \(k_i\leq\ceil{\log_2(n + 1)}\). So \(f(\vx) = \sum_{i}\theta_ig_i(\vx)\) where \(\theta_i\in\set{\pm 1}\).

\Com This reduces the growth of absolute value nesting to logarithmic growth, instead of linear growth.

\todo{what does this inequality say exactly? What is \(k_i\)?}

\sep

\Cor P.w. linear functions are dense in \(C(\R^n)\).

\subsubsection{Maxout Networks}

In 2013 \(k\)-Hinges were re-discovered under the name of \emph{Maxout} by Goodfellow et al.

\sep

\Def[Maxout] is just the \(\max\) non-linearity applied to \(k\) groups of linear functions. So the input \([1:d]\) (of the previous layer) is \emph{partitioned} into \(k\) sets \(A_1,\ldots,A_k\), and then we define the activations \(G_j(\vx)\) for \(j\in\set{1,\ldots,k}\) as

\( G_j(\vx) = \max_{i\in A_j}\set{\vw_i^\T\vx + b_i} \qquad(i\in\set{1,\ldots,d}) \)

\Com So, here we apply the nonlinearity in \(\R^d\) (among some set members \(A_j\)) instead applying the nonlinearity in \(\R\) (as with ridge functions).

\sep

\Thm[Goodfellow, 2013] Maxout networks with two maxout units that are applied to \(2m\) linear functions are universal function approximators.