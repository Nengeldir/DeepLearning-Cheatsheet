\section{Approximation Theory}

\subsection{Compositions of Maps}

\Def[Linear Function] A function \(f\colon\R^n\to\R^m\) is a linear function if the following properties hold
\begin{itemize}
  \item \(\displaystyle\forall\vx,\vx'\in\R^n\colon f(\vx + \vx') = f(\vx) + f(\vx')\)
  \item \(\displaystyle\forall\vx\in\R\,\forall\alpha\in\R\colon f(\alpha\vx) = \alpha f(\vx)\)
\end{itemize}

\sep

\Thm \(f\colon\R^n\to\R\text{ is linear }\Longleftrightarrow f(\vx) = \vw^\T\vx\) for some \(\vw\in\R^n\)

\sep

\Def[Hyperplane]

\(H: =\dset{\vx}{\scprod{\vw,\vx - \vp} = 0} = \dset{\vx}{\scprod{\vw,\vx} = b}\)

where \(b = \scprod{\vw,\vp}\). \(\vw = \)normal vector, \(\vp\) points onto a point on the plane.

\subsection{Universal Approximation}

\Thm[Universal Approximation Theorem] MLPs with one hidden layer and a non-polynomial activation function can approximate any continuous function on a compact subset of \(\R^d\) to arbitrary accuracy \(\epsilon > 0\). Let \(\varphi\) be a non-constant, bounded, and continuous activation function (e.g., Sigmoid, ReLU). There exist weights \(v, w, b\) and an integer \(N\) (number of neurons) such that the network output \(F(x)\) satisfies the condition:

\[|f(x) - F(x)| < \epsilon, \quad \forall x \in K\]

\textbf{Uniform Approximation}: This specific type of convergence guarantees that the maximum error across the \textit{entire} domain \(K\) is bounded by \(\epsilon\). It is stricter than pointwise approximation.

\Thm[Weierstrass Approximation]
Let $f$ be a continuous real-valued function defined on a closed interval $[a, b]$. For every $\epsilon > 0$, there exists a polynomial $P(x)$ such that for all $x \in [a, b]$:
$$ |f(x) - P(x)| < \epsilon $$
\textit{Implication:} Polynomials are dense in the space of continuous functions $C[a,b]$. This is the foundation for Universal Approximation, as neural networks can approximate polynomials.

\sep

\Thm[Dimension Lifting (Leshno's Theorem)]
By the universal approximation theorem, we know that we can approximate $C(\R)$ with a single hidden layer of ReLUs. The lifting theorem then allows us to lift the dimension of the function space to a higher dimension, i.e. $\text{span}(\{\phi(ax+b): a,b \in \mathbb{R}\})$ universally approximates $C(\mathbb{R}^n)$.

\sep

\Thm[Montufar (Linear Regions)]
The number of linear regions carved out by a Deep ReLU Network grows exponentially with depth $L$, but only polynomially with width $n$. For a network with $L$ layers and width $n$ (where $n \ge d$):
$$ \#\text{Regions} = O\left( \left(\frac{n}{d}\right)^{(L-1)d} n^d \right) $$

\Com Deep networks are exponentially more expressive (in terms of complex decision boundaries) than shallow networks of the same parameter count.

\sep

\Thm[Shekhtman (ReLU Basis)]
Any continuous piecewise linear function $g(x)$ on $[0,1]$ (a polygonal line) with breakpoints (knots) $t_1 < \dots < t_k$ can be represented exactly as a weighted sum of ReLU units:
$$ g(x) = a + bx + \sum_{i=1}^k c_i \text{ReLU}(x - t_i) $$

\Com A single hidden layer of ReLUs acts as a universal basis for 1D splines. This establishes the theoretical link between splines and ReLU networks.

\Thm[Barron's Theorem]
For a function $f$ with a finite Fourier moment $C_f$ (a measure of smoothness), there exists a neural network $f_n$ with one hidden layer of $n$ sigmoidal units such that the integrated squared error is bounded by:
$$ ||f - f_n||_{L_2}^2 \leq \frac{(2C_f)^2}{n} $$

\Com The approximation error decays at a rate of $O(1/\sqrt{n})$, which is independent of the input dimension $d$. This suggests neural networks can overcome the "Curse of Dimensionality" for specific classes of functions.

\sep

\Thm[Zaslavsky's Theorem]
The number of distinct regions $R(m, d)$ created by an arrangement of $m$ hyperplanes in a $d$-dimensional space is bounded by:
$$ R(m, d) \leq \sum_{i=0}^{d} \binom{m}{i} $$

\Com In the context of ReLU networks, this bounds the number of linear regions a single layer with $m$ neurons can create. For large $m$, this approximates to $O(m^d)$, showing polynomial growth with width (contrast with Montufar's exponential growth with depth).

\subsubsection{Maxout Networks}

In 2013 \(k\)-Hinges were re-discovered under the name of \emph{Maxout} by Goodfellow et al.

\sep

\Def[Maxout] is just the \(\max\) non-linearity applied to \(k\) groups of linear functions. So the input \([1:d]\) (of the previous layer) is \emph{partitioned} into \(k\) sets \(A_1,\ldots,A_k\), and then we define the activations \(G_j(\vx)\) for \(j\in\set{1,\ldots,k}\) as

\( G_j(\vx) = \max_{i\in A_j}\set{\vw_i^\T\vx + b_i} \qquad(i\in\set{1,\ldots,d}) \)

\Com So, here we apply the nonlinearity in \(\R^d\) (among some set members \(A_j\)) instead applying the nonlinearity in \(\R\) (as with ridge functions).

\sep

\Thm[Goodfellow, 2013] Maxout networks with two maxout units that are applied to \(2m\) linear functions are universal function approximators.