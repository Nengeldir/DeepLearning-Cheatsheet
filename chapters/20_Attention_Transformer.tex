\section{Attention \& Transformers}

% Attention: Token Paradigm, Attention mixing, Query-key matching
We generally work under the \textbf{token paradigm} (sequence of tokens in embedding-form).
\[
x_1,\dots,x_T,\qquad
X = [x_1,\dots,x_T] \in \mathbb{R}^{n\times T}
\]\\
The fundamental idea is
to map the non-contextualized (or less-contextualized) embeddings to contextualized (or
more-contextualized) representations.
\[
\xi_1,\dots,\xi_T,\qquad
\Xi = [\xi_1,\dots,\xi_T] \in \mathbb{R}^{m\times T}
\]\\

\sep
\textbf{Scaled Dot-Product (Keyâ€“Value) Attention Mechanism} produces numbers $\alpha_{s t}$ that are then used to convexly combine the input representation into a new one.\\
\Def[Attention weights] $\alpha_{s t}$ cleverly indexes the "memory" of the model and retrieves the important bits.
Note that attention weights have a \textbf{source} (where attention emerges, index s) and a \textbf{target}
(where attention extracts information, index t). with Value matrix $W$.
\[
\xi_s \;\equiv\; \sum_t \alpha_{s t}\,W x_t,\qquad
\alpha_{s t} \ge 0,\qquad
\sum_t \alpha_{s t} = 1
\]\\
\Def[Attention Matrix] summarize the Attention weights
\[
A = (a_{s t}) \in \mathbb{R}^{T\times T},\qquad
\text{s.t.}\quad \Xi = W X A^\top
\]\\
\sep
\textbf{Bi-Linear Query-Key Matching Function} Query matrix $U_Q$, key value $U_K$ project X to a query and key matrix\\
\[
Q = U_Q X,\qquad K = U_K X,\qquad U_Q,\,U_K \in \mathbb{R}^{q\times n}
\]\\
Produce \Def[Matching Matrix] via \textbf{inner products} of queries and keys
\[
Q^\top K \;=\; X^\top U_Q^\top U_K X \in \mathbb{R}^{T\times T},\qquad
\operatorname{rank}(Q^\top K) \le q
\]\\
The matching matrix already matches the dimensions of the attention matrix. But is commonly passed through a softmax (to normalize \& guarantee non-negativity).\\
\[
A = \operatorname{softmax}\big(\beta\,Q^\top K\big),\qquad
a_{s t} \;=\; \frac{\exp\!\big(\beta\,[Q^\top K]_{s t}\big)}{\sum_r \exp\!\big(\beta\,[Q^\top K]_{s r}\big)}
\]\\
Usually $\beta = \frac{1}{\sqrt{q}}$. This facilitates training (through variance stabilizing scaling).\\
% \sep

% Transformers: Multi-headed attention, Feature transformation, Depth & position, Masked and cross attention, 
\subsection{Transformers}
\Def[Transformer] \\ is an architecture that takes the idea of attention to the extreme. They are computationally efficient in some aspects\\
\begin{center}
    \includegraphics[width=0.9\linewidth]{img/why transformers.png}
\end{center}
\Def[Multi-Headed Attention] \\ 
% runs multiple attention mechanisms in parallel ($r$ \textbf{heads}). Replicate matrices $U_k, U_Q, W$, $r$ times, 
% perform the attention-based propagation \(X \mapsto \Xi_i \qquad (1 \le i \le r)\) and then concatenate the matrices $\Xi_i$ along the feature dimension.\\
\begin{itemize}
    \item runs multiple attention mechanisms in parallel ($r$ \textbf{heads})
    \item Replicate matrices $U_k, U_Q, W$, $r$ times, perform the attention-based propagation \(X \mapsto \Xi_i \qquad (1 \le i \le r)\) and then concatenate the matrices $\Xi_i$ along the feature dimension
    \item each head itself is a key-value attention
\end{itemize}
\[
G\!\big(\xi,\,(x^t,z^t)_{t=1}^s\big)
= W
\begin{bmatrix}
F_1\!\big(\xi,(x^t,z^t)\big)\\[4pt]
\vdots\\[4pt]
F_h\!\big(\xi,(x^t,z^t)\big)
\end{bmatrix}
\]
\[
F_j\!\big(\xi,(x^t,z^t)\big)
= F\!\bigl(W_j^q \,\xi,\; (W_j^x x^t,\; W_j^z z^t)\bigr)
\]
\Thm[Feature Transformation]\\
In order to
gain more representational power, one also needs to allow for a learnable transformation
of features. 
Transformers do this following the network-within-network design pattern by
applying a per-token transformation with a feedforward network (MLP).
$F_\theta$ is an MLP applied columnwise to $\Xi$:
\[
X \mapsto \Xi \mapsto F_\theta(\Xi),\qquad
F_\theta(\Xi) = \big(F_\theta(\xi_1),\dots,F_\theta(\xi_T)\big).
\]\\
When we add resiudal connections and normalization layers, we arrive at a typical encoder block of a Transformer.\\
\begin{center}
    \includegraphics[width=0.35\linewidth]{img/Transformer_encoder_block.png}
\end{center}
\Thm[Depth]\\
Stack transformer blocks in a compositonal manner. One block relies on the per token output of the previous.
typically depth is between 6 \& 10.\\
\Thm[Position Encoding]\\
Transformer architecture is \textbf{permutation invariant}. To overcome this, we need to inject information about the position of tokens in the sequence.\\
This can either be fixed or learned. A common fixed choice is the \textbf{K sinusoidal-function} encoding:
\[
p_{t k} = \begin{cases}
\sin\big(t\,\omega_k\big), & k\ \text{even},\\[4pt]
\cos\big(t\,\omega_k\big), & k\ \text{odd},
\end{cases}\qquad
\omega_k = C^{k/K}\quad (C = 10000).
\]
\begin{center}
    \includegraphics[width=0.9\linewidth]{img/sinusoidal encoding.png}
\end{center}
\Thm[Masked Attention, Decoder]\\
When transformers operate in coupled encoder-decoder pairs the decoder is operated in an autoregressive fashion, where the next output
for the $t$-th token depends on the encoder, but also the produced token representations for
$s < t$. This can be accomplished in a transformer by simply restricting attention to the
past (no masks are worn).\\
\begin{center}
    \includegraphics[width=0.6\linewidth]{img/masked attention.png}
\end{center}
\Def[Cross-Attention] to the representations produced by the encoder. Often decoder and encoder
are of the same depth and cross-attention is implemented across layers of the same depth.
Note that in cross-attention the queries emerge from the decoder, whereas keys and values
are derived from the encoder states.

% LLMs: bayes rule, Neural language models, BERT, Talking LLMS
\subsection{Large Language Models (LLMs)}
LLMs make use of the Bayes' rule paradigm. 
\[
\Pr(\text{target}\mid\text{source}) \propto \Pr(\text{source}\mid\text{target})\,\Pr(\text{target})
\]\\
E.g. in machine translation it is beneficial to train a model mono-lingually (lots of training data).
Thereby we make it easier for the conditional, bi-lingual model.\\
\sep
\Def[Perplexity] is a common measure for LLM accuracy. Lower perplexity indicates better prediction.\\
\[
\mathrm{PPL}(X) \;=\; 2^{H(p,q)}
% \;=\; \exp\!\left\{-\frac{1}{t}\sum_{i=1}^t \log p_\theta(x_i \mid x_{<i})\right\}
\]
where $H(p,q)$ is the \Def[Cross-Entropy],\\
$X = (x_0, x_1, \dots, x_t)$ is a tokenized sequence, $p_\theta(x_i \mid x_{<i})$ is the model's predicted probability 
of token $x_i$ given all previous tokens, and $t$ is the sequence length.\\
% \[
% \mathrm{PPL} \;=\; 2^{H(p,q)}
% \;=\; 2^{-\frac{1}{N}\sum_{i=1}^N \log_2 q(x_i)}
% \]\\
\[
H(p, q) \;=\; -\mathbb{E}_{x \sim p}\big[\log q(x)\big]
\;=\; -\frac{1}{N}\sum_{i=1}^N \log q(x_i)
\]\\
where $p$ is the true data distribution, $q$ is the model's predicted distribution, and $N$ is the sequence length.\\
\sep
Bidirectional Encoder Representations from Transformers \Thm[BERT]
% uses CLS & SEP tokens, subword tokenization, (self-supervised learning & sentence pair classification), BERT is not autoregressive -> mask 15 % of the total words(80% mask, 10% random token, 10% unchanged) 
\begin{itemize}
    \item is a transformer just bigger (24 blocks, 16 heads, 1024 latent space dim., 340M param.)
    \item task-independent basis ("Pre-Training") that enables lots of NLP tasks ("Fine-Tuning") $\rightarrow$ game-changer
    \item subword tokenization \& special tokens (CLS, SEP)
    \item BERT is not autoregressive $\rightarrow$ mask 15\% of the total words (80\% mask, 10\% random token, 10\% unchanged)
    \item trained on two tasks: Self-supervised learning \& Sentence pair classification
\end{itemize}
\sep
embeddings from language model \Thm[ELMo] is a word embedding method for representing a sequence of words as a corresponding sequence of vectors.
\begin{itemize}
    \item Character-based model: morphology, OOV
    \item Left-to-right stacked LSTM (summarize left context of each word)
    \item Right-to-left stacked LSTM (summarize right context of each word)
    \item Stacked LSTM layers, shared character embeddings and output embeddings
    \item collapse all layers 2L + 1 linearly (task-specific weights)
\end{itemize}
\begin{center}
    \includegraphics[width=0.8\linewidth]{img/ELMo.png}
\end{center}
\sep
\Thm[Generative Pre-trained Transformers (GPT)]\\
% relies on the decoder part of the transformer architecture. GPT models have been trained with masked attention on next token
% prediction tasks.
\begin{itemize}
    \item relies on decoder part of transformer architecture
    \item trained with masked attention on next token prediction tasks
    \item prompts control generation. But at the heart is usually an alignment process, typically based on large amounts of human feedback.
    \item powerful zero-shot \& few-shot learning capabilities
\end{itemize}
\subsection{Vision Transformers}
% Vision Transformers: 
\begin{itemize}
    \item use $16 \times 16$ non-overlapping pixel-patches as tokens
    \item flatten patches $\text{p}$ and linearly project to embedding space
        \[
        \text{p}^t \in \mathbb{R}^{p\times p \times q}
        \longmapsto
        x^t \equiv V\,\operatorname{vec}(\text{p}^t) \in \mathbb{R}^n,
        V \in \mathbb{R}^{\,n\times (q p^2)}
        \]\\
    \item pre-processing ignores the 2D structure of images (unproblematic for large datasets)
    \item no built-in translation equivariance like in CNNs, but lower inductive bias \& little spatial awareness
\end{itemize}
\begin{center}
    \includegraphics[width=0.28\linewidth]{img/VIT block.png}
\end{center}