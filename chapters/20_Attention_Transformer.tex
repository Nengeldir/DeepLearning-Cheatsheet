\section{Attention \& Transformers}

% Attention: Token Paradigm, Attention mixing, Query-key matching

% Transformers: Multi-headed attention, Feature transformation, Depth & position, Masked and cross attention, 

% LLMs: bayes rule, Neural language models, BERT, Talking LLMS

% Vision Transformers: 
% -----------------------------------

% Seq-to-Seq (with attention), Thought vetors, key-value attention, Dot-product attention, multi-headed attention

% Transformers: Encoder, positional encoding, sinusoidal encoding, MLP, Decoder, why?

% ELMo (Word2vec)

% GPT / Zero-shot learning