\section{Optimization}
Optimization is the process of finding the best solution to a problem.

\Thm We have the following chain of inclusions for functions over a \emph{closed} and \emph{bounded} (i.e., compact) subset of the real line.

Continously differentiable \(\subseteq\) Lipschitz continuous \(\subseteq\) (Uniformly) continous


\sep

\Thm If we use Nesterov acceleration (in the general case), then we get a polynomial convergence rate of \(\BigO(t^{-2})\).

\Com The trick used in the Nesterov approach is \emph{momentum}.

\subsection{Optimization Challenges in NNs: Curvatures}

%Check here
\begin{center}
  \includegraphics[width=1\linewidth]{img/challenges-curvature-and-gradients}
\end{center}

\subsubsection{Convergence Rates}

Under certain conditions SGD converges to the optimum:

\begin{itemize}
  \item If we have a convex, or strongly convex objective,
  \item and if we have Lipschitz continous gradients,
  \item and a deaying learning rate, s.t.
  \[\underbrace{\sum_{t = 1}^\infty\eta_t = \infty}_{\mathclap{\text{we get far enough}}}, \qquad \underbrace{\sum_{t = 1}^\infty\eta_t^2<\infty}_{\text{our steps get always smaller}}\]
  typically \(\eta_t = Ct^{-\alpha}\), \(\frac{1}{2}<\alpha<1\) (c.f. harmonic series.)
  \item or we use iterate (Polyak) averaging (once we start jumping around, we average the solutions over time).
\end{itemize}

Then, we can get the following convergence rates:

\begin{itemize}
  \item strongly-convex case: can achieve a \(\BigO(1/t)\) suboptimality rate (only polynomial convergence)
  \item non-strongly convex case: \(\BigO(1/\sqrt{t})\) suboptimality rate (even worse than polynomial convergence)
\end{itemize}

So, even if the convergence rates are not super nice, thanks to the cheap gradient computation (only one example at the time), we may even converge faster than computing the gradient on the full dataset everytime.
