\section{Regression}
\textbf{Linear Regression}: $f(x) = w^Tx + b$

$$\ell(w) = \frac{1}{2n}\sum_{i=1}^n(y_i - f(x_i))^2$$

\textit{Analytically solvable}: $w^* = (X^TX)^{-1}X^Ty$

\sep

\textbf{Ridge Regression}: $f(x) = w^Tx + b$ with $\ell_2$ regularization
$$\ell(w) = \frac{1}{2n}\sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\|w\|^2$$
\textit{Analytically solvable}: $w^* = (X^TX + \lambda I)^{-1}X^Ty$

\sep

\textbf{Lasso Regression}: $f(x) = w^Tx + b$ with $\ell_1$ regularization
$$\ell(w) = \frac{1}{2n}\sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\|w\|_1$$

\sep

\textbf{Logistic Regression}: $f(x) = \sigma(w^Tx + b)$

It minimizes the cross-entropy loss (negative log-likelihood), which acts as a surrogate loss for the 0/1 classification error.

$$\ell(w) = -\sum_{i=1}^n(y_i\log(\sigma(w^Tx_i)) + (1-y_i)\log(1-\sigma(w^Tx_i)))$$
