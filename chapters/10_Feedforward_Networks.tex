\section{Feedforward Networks}
\subsection{}
\Def[Feedforward Network] is a set of computational units arranged in a DAG (layer-wise processing)

\( F = F^{L}\circ \cdots \circ F^{1} \)

where each layer \(l\in\set{1,\ldots,L}\) is a composition of the following functions

\(F^{l}\colon \R^{m_{l - 1}}\to\R^{m_l}\) \quad \(F^{l} = \sigma^l\circ \overline{F}^l\)

where \(\overline{F}^l\colon \R^{m_{l - 1}}\to\R^{m_l}\) is the linear function in layer \(l\)

\( \overline{F}^l(\vh^{l - 1}) = \MW^l\vh^{l - 1} + \vb, \quad \MW^l\in\R^{m_{l}\times m_{l - 1}}, \quad \vb\in\R^{m_l} \)

and \( \sigma^l\colon\R^{m_l}\to\R^{m_l} \) element-wise non-linearity at layer \(l\).

Note that \(\vh^0: =\vx\).

\sep

\Thm Feedforward Networks are invariant under permutations of units: The units within a hidden layer are interchangeable (along with their corresponding weights).

\Thm The units of feedforward networks are invariant along directions orthogonal to the weight vector $F[\vw, \vb](\vx) = F[\vw, \vb](\vx + \delta \vx): \forall \delta\vx \perp \vw$

\Thm A layer of width $m$ can be embedded into a layer of width $m+1$ by adding "barren" (unused) units or by cloning existing units.

\sep

\subsection{Empirical \& Population Risk}
In learning, we are interested in minimizing the population risk (test risk). We use the training risk (empirical risk) as an estimate of the population risk.

\Def[Expected (training) Risk of a Function \(F\)]

\(\ell[\theta](S) = \frac{1}{N}\sum_{i = 1}^N \ell[\theta](x_i, y_i)\)

\Def[Population Risk/Test Risk]

\(\cR(\ell[\theta];p) = \mathbb{E}_{(\vx,y)\sim p}\ell(\vx,y)\)

\sep

\subsubsection{Linear Networks}
Linear networks are composed of linear maps (with activation function $\phi = \id$). However, they do not gain representational power from depth because affine maps are closed under composition. They are primarily used for dimensionality reduction (contraction), c.f. Autoencoders.

\subsection{Autoencoders}

\textbf{Goal:} \emph{Compress} the data into \(m\)-dim. \((m\leq d)\) representation.

\Def[Autoencoder] tries to learn the identity function.

\[\cR(\theta) = \frac{1}{2n}\sum_{i = 1}^n\norm{\vx - F_\theta(\vx)}_2^2 = \Exp[\vx\sim p_{\text{emp}}]{\ell(\vx,(H\circ G)(\vx))}\]

Typically, a linear autoencoder can be broken into two parts \(G\) and \(H\) such that
\(F = H\circ G\approx \vx\mapsto\vx\), where \(G \in \R^{m\times n}\) is the encoder and \(H \in \R^{n\times m}\) is the decoder.

The representation of \(G\) and \(H\) is not unique and by the Eckhart-Young theorem we can find the optimal representation (under the squared reconstruction error) by performing the SVD of the data matrix \(\MX\) (PCA).

\[\MX = 
\begin{bmatrix}
    \vertbar & \vertbar & & \vertbar\\
    \vx_1 & \vx_2 & \cdots & \vx_k\\
    \vertbar & \vertbar & & \vertbar\\
\end{bmatrix}\]
is of the following form:

\[\MX = \underset{n\times n}{\MU} \underbrace{\diag^\dagger(\sigma_1,\ldots,\sigma_{\min(n,k)})}_{=:\MSigma\in\R^{n\times k}}\underset{k\times k}{\MV^\T}.\]

And the matrices \(\MU\) and \(\MV\) are orthogonal; we have an orthogonal basis. 

\sep

\Thm[Non-uniqueness] For any invertible matrix \(\MA\) we have

\[\underbrace{(\MU_m\MA^{-1})}_{\tilde{\MD}} \underbrace{(\MA\MU_m^\T)}_{\tilde{\MC}} = \MU_m\MU_m^\T\]

\ssep
\Thm[Eckhart-Young] For \(m\leq\min(n,k)\) and the objective

\[\argmin_{\hat{\MX}\colon\rank(\hat{\MX}) = m}\norm{\MX - \hat{\MX}}_F^2 = \MU_m\diag(\sigma_1,\ldots,\sigma_m)\MV_m^\T\]

where the subscript \(m\) refers to the matrices of the SVD pruned to \(m\) columns.

\ssep

\Cor This means that a linear auto-encoder with \(m\) hidden units cannot improve the SVD since \(\rank(\MC\MD)\leq m\). However, the auto-encoder can achieve the result of the SVD.

% \subsubsection{Non-Linear Autoencoders}

% Non-linear autoencoders allow us to learn powerful non-linear generalizations of the PCA.

% \sep

% \Def[Non-Linear Autoencoder] contains many hidden layers with nonlinear-activation functions as we want (as long as there's a bottleneck layer) and train the parameters via MLE.

% \subsubsection{Regularized Autoencoders}

% One may also regularize the code \(\vz\) via a regularizers \(\Omega(\vz)\). This will give us a regularized autoencoder.

% There are various flavours of regularization:
% \begin{itemize}
%     \item standard \(L_2\) penalty: ability to learn ``overcomplete'' codes
%     \item \Def[Code Sparseness] e.g., via \(\Omega(\vz) = \lambda\norm{\vz}_1\)
%     \item \Def[Contractive Autoencoders] \(\Omega(\vz) = \lambda\norm{\frac{\partial\vz}{\partial\vx}}_F^2\). This penalizes the Jacobian and generalizes weight decay (cf. Rifai et al, 2011)
% \end{itemize}

% \subsubsection{Denoising Autoencoders}

% Autoencoders allso allow us to separate the signal from noise: De-noising autoencoders aim to learn features of the original data representation that are robust under noise.

% \sep

% \Def[Denoising Autoencoder] we perturb the inputs \(\vx\mapsto\vx_{\veta}\), where \(\eta\) \emph{is a random noise vector}, e.g., additive (white) noise 

% \[\vx_\eta = \vx + \veta, \qquad \veta\sim\cN(\vo,\sigma^2\MI) \]

% and instead of the original objective, we minimize the following

% \[\Exp[\vx]{\Exp[\veta]{\ell(\vx,(H\circ G)(\vx_{\veta}))}}\]

% The hope is that we'll achieve \emph{de-noising}, which happens if

% \[\norm{\vx - H(G(\vx_{\veta}))}^2 < \norm{\vx - \vx_{\veta}}^2\]

% So this would mean that the reconstruction error of the noisy data is less than the error we created by the noise we've added (then the de-noising works).



% Wish: \(\cR(\hat{F};\cS_n)\stackrel{n\to\infty}{\to}\cR^*(F^*)\) (no overfitting)

% \sep

% \Def[Regularized Empirical Risk]

% \( \cR_r(F;\cS_n) := \cR(F;\cS_n) + \lambda\Omega(\norm{F}) \)
% 

\subsection{Residual Networks}
\Def{Residual Networks} learn an incremental improvement: \(F(x) = x + [\phi(Wx + b)]\). They utilize "skip connections to propagate inputs forward, allowing to mitigate the vanishing/exploding gradient problem. It allows us to increase depth even more. (Important paper: ResNet 2015). The skip connection can also be transformed with a matrix \(\MA\) to allow for more flexibility \(F(x) = \MA x + [\phi(Wx + b)]\).

\subsubsection{Regularization Approaches}

\Def{Weight Decay ($L_2$ Regularization)}: Adds a penalty to the loss function proportional to the square of the weights' magnitude to constrain complexity.
$$\ell(\theta) = \ell_{emp}(\theta) + \frac{\lambda}{2} \|\theta\|^2$$

\sep

\Def{Data Augmentation}: Artificially expands the training set by creating modified versions of existing data (e.g., flipping, rotating) to teach invariance.

\sep

\Def{Noise Robustness}: Injects random noise into inputs, weights, or gradients to make the model insensitive to small perturbations.

\sep

\Def{Semi-supervised Learning}: Uses a small set of labeled data alongside unlabeled data. Unlabeled data helps learn the distribution $P(x)$ to regularize $P(y|x)$.

\sep

\Def{Dropout}: Prevents co-adaptation by randomly deactivating neurons.
\begin{itemize}
    \item \textbf{Training}: Each neuron is kept active with probability $p$ (and dropped with $1-p$). This is mathematically applied using a mask vector $\mathbf{m}$:
    $$ y = \mathbf{m} \odot h, \quad \text{where } \mathbf{m} \sim \text{Bernoulli}(p) $$
    \item \textbf{Inference (Weight Scaling)}: During testing, all neurons are active. To match the expected output magnitude from training (since $E[y_{train}] = p \cdot h$), we must scale the weights down:
    $$ W_{test} = p \cdot W_{train} $$
\end{itemize}

\sep

\Def{Early Stopping}: Monitors validation error and stops training when performance degrades, effectively selecting parameters before overfitting occurs. Rigorous analysis shows that early stopping approximates a form of $L_2$ regularization.

\sep

\Def{Multi-task Learning}: Trains on multiple tasks simultaneously using shared representations, introducing inductive bias that aids generalization.

\sep

\Def{Parameter Sharing}: Constraints the model by forcing sets of parameters to be identical (e.g., filter weights in CNNs).

\sep

\Def{Ensembles}: Combines predictions of multiple independent models to average out individual errors.
$$y_{final} = \frac{1}{k} \sum_{i=1}^{k} M_i(x)$$

\sep

\Def{Batch Normalization (BN)}
A technique to stabilize training by normalizing the inputs to a layer for each mini-batch. It addresses \textit{Internal Covariate Shift} (the changing distribution of layer inputs during training).
\begin{enumerate}
    \item \textbf{Normalization}: Calculate batch mean $\mu_B$ and variance $\sigma^2_B$ to normalize input $x$:
    $$ \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma^2_B + \epsilon}} $$
    \item \textbf{Scale and Shift}: The network learns parameters $\gamma$ and $\beta$ to restore representation power (allows the network to undo the normalization if needed):
    $$ y = \gamma \hat{x} + \beta $$
\end{enumerate}

\textit{Benefits}: Allows higher learning rates, reduces sensitivity to initialization, and acts as a weak regularizer.

\subsubsection{Initialization Techniques}

Proper initialization is crucial to prevent vanishing or exploding gradients at the start of training.

\sep

\Def[LeCun Initialization]
\begin{itemize}
    \item \textbf{Goal}: Keep the variance of the input and output unchanged for efficient backprop in linear networks.
    \item \textbf{Use Case}: SELU activation, or standard Sigmoid (older standard).
    \item \textbf{Variance}: $\text{Var}(W) = \frac{1}{n_{in}}$
\end{itemize}

\sep

\Def[Glorot Initialization (Xavier)]
\begin{itemize}
    \item \textbf{Goal}: Maintains the variance of activations during forward pass and gradients during backward pass. Assumes linear or Tanh/Sigmoid activations.
    \item \textbf{Use Case}: Tanh, Sigmoid, Softmax.
    \item \textbf{Variance}: $\text{Var}(W) = \frac{2}{n_{in} + n_{out}}$
\end{itemize}

\sep

\Def[He Initialization (Kaiming)]
\begin{itemize}
    \item \textbf{Goal}: accounts for the fact that ReLU zeroes out half of the inputs (reducing variance by half). It doubles the variance compared to LeCun to compensate.
    \item \textbf{Use Case}: ReLU, Leaky ReLU.
    \item \textbf{Variance}: $\text{Var}(W) = \frac{2}{n_{in}}$
\end{itemize}

\sep

\Def[Orthogonal Initialization]
\begin{itemize}
    \item \textbf{Goal}: Initialize weight matrices as orthogonal matrices ($W^T W = I$). This ensures that the matrix eigenvalues have absolute value 1.
    \item \textbf{Use Case}: Recurrent Neural Networks (RNNs) and very deep networks. It prevents gradients from vanishing or exploding even after many matrix multiplications (time steps).
\end{itemize}