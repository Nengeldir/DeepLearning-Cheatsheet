\section{Backpropagation \& Optimization}
\subsection{Automatic Differentiation (AD)}

In neural networks we can efficiently compute the gradient of the loss w.r.t. the parameters \(\nabla_\theta\cR\) by exploiting the computational structure of the network.

\sep

\Def[Forward Mode AD] (Forward Propagation of Tangents)
Computes the derivative simultaneously with the function evaluation.
\begin{itemize}
    \item \textbf{Mechanism}: Propagates perturbations $\dot{x}$ forward through the graph. For every node $v_i$, it computes the value $v_i$ and the derivative $\dot{v}_i = \frac{\partial v_i}{\partial x}$.
    \item \textbf{DAG Flow}: Derivatives flow in the \textit{same direction} as data (Input $\to$ Output). No need to store the full graph in memory.
    \item \textbf{Efficiency}: Efficient for functions $f: \mathbb{R}^n \to \mathbb{R}^m$ where $n \ll m$ (few inputs, many outputs).
    \item \textbf{Cost}: requires $n$ passes to compute gradients for $n$ parameters. In Deep Learning (millions of parameters), this is computationally prohibitive.
    \item $O(n)$ passes
\end{itemize}

\sep

\Def[Reverse Mode AD] (Backpropagation)
Computes derivatives after a full function evaluation by traversing the graph in reverse.
\begin{itemize}
    \item \textbf{Mechanism}: First runs a \textit{Forward Pass} to compute and store intermediate values (Primals). Then runs a \textit{Backward Pass} to propagate adjoints $\bar{v}_i = \frac{\partial L}{\partial v_i}$.
    \item \textbf{DAG Flow}: Derivatives flow in the \textit{opposite direction} (Output $\to$ Input). Requires storing the DAG and intermediate activations (memory intensive).
    \item \textbf{Efficiency}: Efficient for functions $f: \mathbb{R}^n \to \mathbb{R}^m$ where $n \gg m$ (many inputs, few outputs).
    \item \textbf{Relevance}: This is the standard in Deep Learning because we map millions of parameters ($n$) to a single scalar loss ($m=1$). It computes all gradients in a single backward pass.
    \item $O(m)$ passes
\end{itemize}

\includegraphics[width=\linewidth]{img/Forward&BackwardMode.png}

\subsection{Jacobian Matrix}

\Def[Jacobi Matrix of a Map] 
The Jacobian \(\MJ_F\) of a map \(F\colon\R^n\to\R^m\) is defined as (Numerator Layout)
\vspace{-3em}
\begin{align*}
    \MJ_F
    :=
    &\begin{pmatrix}
        \horzbar(\nabla F_1)^\T\horzbar \\
        \horzbar(\nabla F_2)^\T\horzbar \\
        \vdots \\
        \horzbar(\nabla F_m)^\T\horzbar
    \end{pmatrix}
    = \\
    &\begin{pmatrix}
        \frac{\partial F_1}{\partial x_1} & \frac{\partial F_1}{\partial x_2} & \cdots & \frac{\partial F_1}{\partial x_n} \\
        \frac{\partial F_2}{\partial x_1} & \frac{\partial F_2}{\partial x_2} & \cdots & \frac{\partial F_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial F_m}{\partial x_1} & \frac{\partial F_m}{\partial x_2} & \cdots & \frac{\partial F_m}{\partial x_n}
    \end{pmatrix} \in\R^{m\times n}
\end{align*}
\vspace{-3em}
\subsection{Gradient Descent}

\Def[Gradient Descent (GD)]
Iteratively moves parameters \(\theta\) in the direction of the negative gradient of the loss function \(J(\theta)\).
\[ \theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t) \]

\Com In Stochastic GD (SGD), the gradient is approximated using a single sample (or mini-batch) to introduce noise and escape local minima. Altough, the gradient is unbiased it adds variance, this can help to escape local minima and saddle points.

\Com Gradient Flow can be seen as the numerical integration of the continuous-time ordinary differential equation (ODE) \(\dot{x} = -\nabla f(x)\).

\Com For an $L$-smooth function, GD with a fixed step size \(\eta \leq \frac{1}{L}\) is optimal.

\sep

\Def[Polyak Averaging (Averaged SGD)]
Instead of using the final parameter vector \(\theta_T\), this method uses the arithmetic mean of the parameters traversed during training.
\[ \bar{\theta}_t = \frac{1}{t} \sum_{i=1}^{t} \theta_i \quad \text{or} \quad \bar{\theta}_t = (1-\beta)\bar{\theta}_{t-1} + \beta \theta_t \]
\textit{Benefits}:
\begin{itemize}
    \item It effectively increases the effective batch size and reduces the variance of the estimate.
    \item Allows the use of larger learning rates (longer steps) while still converging to the optimal solution asymptotically.
    \item Often achieves the optimal convergence rate of $O(1/t)$ for convex problems.
\end{itemize}

\sep

\Def[Learning Rate Condition] (Robbins-Monro Conditions), for Stochastic Gradient Descent (SGD) to guarantee convergence to a local minimum (in non-convex cases) or global minimum (in convex cases), the step size schedule \(\alpha_t\) must satisfy two conditions:

1.\textbf{Explore Forever}: The steps must sum to infinity to ensure the algorithm can reach the optimum from any starting point, no matter how far.
    $$ \sum_{t=1}^{\infty} \alpha_t = \infty $$

2.\textbf{Decay Fast Enough}: The squared steps must sum to a finite value to ensure the variance (noise) of the updates tends to zero, preventing the parameters from oscillating forever around the minimum.
    $$ \sum_{t=1}^{\infty} \alpha_t^2 < \infty $$

\textit{Example}: A schedule of \(\alpha_t = \frac{1}{t}\) satisfies both, whereas \(\alpha_t = \frac{1}{\sqrt{t}}\). satisfies the first but not the second.

\sep

\Def[Momentum]
Accelerates SGD by navigating along the relevant direction and softening oscillations in irrelevant directions. It maintains a velocity vector $v$ (exponential moving average of past gradients).
\[\theta^{t+1} = \theta^{t} - \eta \nabla J(\theta^{t}) + \beta(\theta^{t} - \theta^{t-1})\]
where $\beta \in [0,1)$ is the momentum term (friction).

\sep

\Def[Nesterov Accelerated Gradient] A "look ahead" version of GD, also called NAG. It computes the gradient at the \textit{approximate future position} of the parameters rather than the current position.
\begin{align*}
    \vartheta^{t+1} &= \theta^t + \beta(\theta^t - \theta^{t-1}) \\
    \theta^{t+1} &= \vartheta^{t+1} - \eta \nabla f(\vartheta^{t+1})
\end{align*}

\sep

\Def[Adaptive Learning Rate Methods]
These methods adjust the learning rate for each parameter individually, scaling them based on the history of gradient magnitudes.

\sep
\Def[RMSProp]
Designed to resolve the diminishing learning rates of Adagrad. It uses a decaying average of squared gradients.
\begin{align*}
E[g^2]_t &= \beta E[g^2]_{t-1} + (1-\beta) (\nabla J(\theta_t))^2 \\
\theta_{t+1} &= \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \nabla J(\theta_t)
\end{align*}

\sep
\Def[Adam (Adaptive Moment Estimation)]
Combines Momentum (first moment $m_t$) and RMSProp (second moment $v_t$). It also includes bias correction terms $\hat{m}_t, \hat{v}_t$ to account for initialization at zero.
\begin{align*}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) \nabla J(\theta_t) \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) (\nabla J(\theta_t))^2 \\
\theta_{t+1} &= \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align*}

\sep

\Def[Muon Optimizer]\\
is an optimizer that takes into account the matrix structure of model parameters. Specifically, while optimizing a loss function $\mathcal{L}(W)$
that depends on a weight matrix $W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$ , at iteration t we follow an update:\\
\[
M_t = \mu M_{t-1} + \nabla_W \mathcal{L}(W_t)
\]
\[
P_t = \operatorname{orthogonalize}(M_t)
\]
\[
W_{t+1} = W_t - \eta \sqrt{\frac{d_{\mathrm{out}}}{d_{\mathrm{in}}}}\,P_t.
\]
Muon is motivated by its ability to increase the scale of "rare directions" that are otherwise ignored during learning\\
Orthogonalization might require SVD ($U\Sigma V^T$, with unitary $U\in\mathbb{R}^{m \times m}$, rectangular diagonal matrix $\Sigma\in\mathbb{R}^{m \times n}$, and unitary $V \in \mathbb{R}^{n \times n}$)
