\section{Probability}

Sum Rule \(P(X = x_i) =  \sum_{j = 1}^{J} p(X = x_i,Y = y_i)\)

Product rule \(P(X, Y) =  P(Y|X) P(X)\)

Independence \(P(X, Y) =  P(X)P(Y)\)

Bayes' Rule \(P(Y|X) =  \frac{P(X|Y)P(Y)}{P(X)} = 
\frac{P(X|Y)P(Y)}{\sum\limits^k_{i = 1}P(X|Y_i)P(Y_i)}\)

\sep

Cond. Ind. \( \rX\perp\rY|Z \Longrightarrow \cProb{X,Y}{Z} = \cProb{X}{Z}\cProb{Y}{Z} \)

Cond. Ind. \( \rX\perp\rY|Z \Longrightarrow \cProb{X}{Y,Z} = \cProb{X}{Z} \)

\sep

\(\Exp{\rX} = \int_{\cX} t \cdot f_X(t)\,dt = :\mu_X\)

\(\Cov{X,Y} = \Exp[x,y]{(X - \Exp[x]{X})(Y - \Exp[y]{Y})}\)

\(\text{Cov}(X): =\Cov{X,X} = \Var{X}\)

\(X,Y\) independent \(\Longrightarrow\) \(\Cov{X,Y} = 0\)

\sep

\(\MX\MX^T\geq 0\) (symmetric positive semidefinite)

\(\Var{X} = \Exp{X^2} - \Exp{X}^2\)

\(\Var{\MA\MX} = \MA\Var{\MX}\MA^\T\) \quad \(\Var{aX + b} = a^2\Var{X}\)

\[\Var{\sum_{i = 1}^n a_iX_i} = \sum_{i = 1}^n a_i^2\Var{X_i} +  2\sum_{i<j}a_ia_j\Cov{X_i,X_j} \]

\sep

\(\frac{\partial}{\partial t} \Prob{X\leq t} =  \frac{\partial}{\partial t} F_X(t) =  f_X(t)\)
(derivative of c.d.f. is p.d.f)

\(f_{\alpha Y}(z) =  \frac{1}{\alpha}f_Y(\frac{z}{\alpha})\)

\sep

\Thm The moment generating function (MGF) \(\psi_{X}(t) = \Exp{e^{tX}}\) characterizes the distr. of a rv

\[Be(p) \quad pe^t + (1 - p)\]
\[\cN(\mu,\sigma)\quad \exp\left(\mu t +  \frac{1}{2}\sigma^2 t^2\right)\]
\[Bin(n,p)\quad (pe^t +  (1 - p))^n\]
\[Gam(\alpha,\beta)\quad \left(\frac{1}{a - \beta t}^\alpha\right)\] for \(t<1/\beta\) \\
\[Pois(\lambda)\quad e^{\lambda(e^t - 1)}\]


\sep

\Thm If \(\rX_1,\ldots,\rX_n\) are indep. rvs with MGFs \(M_{\rX_i}(t) = \Exp{e^{t\rX_i}}\), then the MGF of \(\rY = \sum_{i = 1}^n a_i\rX_i\) is \(M_{\rY}(t) = \prod_{i = 1}^n M_{X_i}(a_it)\).

\sep

\Thm Let \(X\), \(Y\) be indep., then the p.d.f. of \(Z = X + Y\) is the conv. of the p.d.f. of \(X\) and \(Y\): \( f_Z(z) =  \int_{\R} f_X(t)f_Y(z - t) \,dt =  \int_{\R} f_X(z - t)f_Y(t) \,dt \)

\sep

\( \cN(\vx;\vmu,\MSigma) =  \frac{1}{\sqrt{(2\pi)^d\det(\MSigma)}} \exp\left(-\frac{1}{2}(\vx - \vmu)^\T\MSigma^{-1} (\vx - \vmu)\right) \)

\( \hat{\mu} = \frac{1}{n}\sum_{i = 1}^n \vx_i \) \quad \( \hat{\MSigma} = \frac{1}{n}\sum_{i = 1}^n (\vx - \hat{\vmu})(\vx - \hat{\vmu})^\T \)

\sep

\Thm \(
    \Prob{
        \begin{bmatrix}
            \va_1 \\
            \va_2 \\
        \end{bmatrix}
    }
    = 
    \cDist{\cN}{
        \begin{bmatrix}
            \va_1 \\
            \va_2 \\
        \end{bmatrix}
    }{
        \begin{bmatrix}
            \vu_1 \\
            \vu_2 \\
        \end{bmatrix},
        \begin{bmatrix}
            \MSigma_{11} & \MSigma_{12} \\
            \MSigma_{21} & \MSigma_{22}
        \end{bmatrix}
    }
\) \\
\(\va_1,\vu_1\in\R^{e}\), \(\MSigma_{11}\in\R^{e\times e}\) p.s.d.
\(\MSigma_{12}\in\R^{e\times f}\) p.s.d.\\
\(\va_2,\vu_2\in\R^{f}\), \(\MSigma_{22}\in\R^{f\times f}\) p.s.d.
\(\MSigma_{21}\in\R^{f\times e}\) p.s.d.\\
\begin{align*}& \cProb{\va_2}{\va_1 = \vz} = \\ &\cDist{\cN}{\va_2}{\vu_2 + \MSigma_{21}\MSigma_{11}^{-1}(\vz - \vu_1),\MSigma_{22} - \MSigma_{21}\MSigma_{11}^{-1}\MSigma_{12}}\end{align*}

\sep

\Thm[Chebyshev] Let \(X\) be a rv with \(\Exp{X} = \mu\) and variance \(\Var{X} = \sigma^2<\infty\). Then for any \(\epsilon > 0\), we have \( \Prob{\abs{X - \mu}\geq\epsilon} \leq\frac{\sigma^2}{\epsilon^2}. \)

\todo{Luca: Add more probability distributions}