\section{Generative Models}

In unsupervised learning our goal is to learn some underlying hidden
\emph{structure} of the data (clustering, dimensionality reduction, feature
learning, density estimation). Now, generative modeling has the following goal:

\textbf{Goal:} given data \(D\), generate new samples from the same
distribution \(p_{\text{data}}\). We want to learn \(p_{\text{model}}\) similar to
\(p_{\text{data}}\).

The nice thing is that the traning data is cheap, as we need no labels. However,
it's a hard task.

In some way or another, any generative model has to cope with density
estimation (which is the hard task). This problem is tackled in different
ways by the several flavours of generative models:

\begin{center}
  \includegraphics[width=0.7\linewidth]{%
img/taxonomy-of-generative-models}
\end{center}

\begin{itemize}
  \item \textbf{explicit density estimation:} explicitly define and solve for
  \(p_{\text{model}}(\vx)\)
  \begin{itemize}
    \item \textbf{tractable} we can comute \(p_{\text{model}}(\vx)\)
    \item \textbf{approximate} we approximate \(p_{\text{model}}(\vx)\) in some
    way
  \end{itemize}
  \item \textbf{implicit density estimation:} learn a model that can sample from
  \(p_{\text{model}}(\vx)\) without explicitly defining it.
\end{itemize}

\subsection{Noise Contrastive Estimation (NCE)}

Approach: Reduce unsupervised problem of estimating \(p(\vx)\) to binary
classification problem. The MLE of the classification problem is asymptotically
consistent with estimator of original problem. Asymptotically consistent: with
increasing number of datapoints, the resulting estimates become more and more
concentrated near the true value of the estimated parameter.

\sep

Noise contrastive estimation is an explicit density estimation method. It works
by turing a unnormalized probability distribution into a normalized probability
distribution. Instead of computing the partition function NCE solves the
normalization probelm by extending everything to a joint distribution which has
a switch variable that selects either the real distribution or a noise
distribution. The same thing is done for the training distribution. Then
everything is trained using MLE.

\sep

Many probabilistic models are defined by an unnormalized probability
distribution \(\tilde{p}(\vx;\theta)\). We must normalize \(\tilde{p}\) by dividing
by a partition function \(Z(\theta)\) to obtain a valid probability distribution

\(
p(\vx;\theta)
= 
\frac{1}{Z(\theta)}\tilde{p}(\vx;\theta).
\)

The partition function is an integral or sum over the unnormalized probability
of all states: \(Z(\theta) = \int_{\cX} \tilde{p}(\vx)\,d\vx\). This operation is
intractable in many cases.

So what makes learning undirected models by maximum likelihood particularly
difficult is that the partition function depends on the parameters. Thus, the
gradient of the log-likelihood w.r.t. the parameters has a term corresp. to the
gradient of the partition function:

\(
\nabla_{\theta}\log(p(\vx;\theta))
= 
\nabla_{\theta}\log(\tilde{p}(\vx;\theta))
- \nabla_{\theta}\log(Z(\theta)).
\)

This is a well-known decomposition into the \emph{positive phase} and the
\emph{negative phase} of learning.

\sep

Most techniques for estimating modesl with an intractable partition function do
not provide an estimate of the partition function.

\textbf{Noise Constrastive Estimation (NCE)} takes a different strategy: In this
approach, the probability distribution estimated by the model is represented
explicitly as

\(
\log(p_{\text{model}}(\vx))
= 
\log(\tilde{p}(\vx;\theta)) + \underbrace{c}_{\mathclap{= -\log(Z(\theta))}}.
\)

So we have the relationship

\(
\frac{1}{Z(\theta)} = e^{c}.
\)

Now, rather than estimating only \(\theta\), NCE treats \(c\) as just another
parameter and estimates \(\theta\) and \(c\) simultaneously, using the same
algorithm for both. The resulting \(\log(p_{\text{model}}(\vx))\) may thus not
correspond exactly to a valid probability distribution, but it will become
closer and closer to being valid as the estimate of \(c\) improves.

We cannot optimize this with MLE, because MLE would just set \(c\) arbitrarily
high. (we want: \(c\) to be s.t. we get a valid probability distr.)

NCE works by reducing the unsupervised learning problem of estimating \(p(\vx)\)
to that of learning a probabilistic binary classifier in which one of the
categories corresponds to the data generated by the model. This supervised
learning problem is constructed in such a way that the MLE defines an
asymptotically consistent estimator of the original problem.

Specifically, we introduce a second distribution, the \emph{noise contrastive
distribution} \(p_\text{noise}(\vx)\). The noise distribution should be tractable
to evaluate and to sample from. We can now construct a model over both \(\vx\) and
a new \emph{binary} class variable \(y\sim Be(p)\). In the new joint probability
model we specify that
\begin{align*}
p_{\text{joint}}(y=1)&=p_{\text{joint}}(y=0)=\frac{1}{2}
\\
p_{\text{joint}}(\vx|\vy=1)&=p_{\text{model}}(\vx)=e^c\tilde{p}(\vx;\theta)
\\
p_{\text{joint}}(\vx|\vy=0)&=p_{\text{noise}}(\vx)
\end{align*}
In other words, \(y\) is a switch variable that determines whether we will
generate \(\vx\) from the model or from the noise distribution.

We can construct a similar joint model \(p_{\text{train}}\) of the training data.
In this case, the switch variable \(y\) determines whether we draw \(\vx\) from the
data or from the noise distribution. Formally,
\begin{align*}
p_{\text{train}}(y=1)&=p_{\text{train}}(y=0)=\frac{1}{2}
\\
p_{\text{train}}(\vx|y=1)&=p_{\text{data}}(\vx)
\\
p_{\text{train}}(\vx|y=0)&=p_{\text{noise}}(\vx)
\end{align*}
We can now just use standard maximum likelihood learning on the
\emph{supervised} learning problem of fitting \(p_{\text{joint}}\) to
\(p_{\text{train}}\).
\[
(\theta^*,c^*)
= 
\argmax_{\theta,c}
\Exp[\vx,y\sim p_{\text{train}}]{\log\left(p_{\text{joint}}(y|\vx)\right)}.
\]
\todo{Which distribution is used here in the expectation? Conditional or joint?}

The distribution \(p_{\text{joint}}\) is essentially a logistic regression model
applied to the difference in log probabilities of the model and the noise
distribution:
\begin{gather*}
\begin{align*}
p_{\text{joint}}(y=1|\vx)
&=\frac{p_{\text{model}}(\vx)}{p_{\text{model}}(\vx)+p_{\text{noise}}(\vx)}
\\
&=\frac{e^c\tilde{p}_{\text{model}}(\vx)}{
e^c\tilde{p}_{\text{model}}(\vx)+p_{\text{noise}}(\vx)}
\\
&=\sigma(\log(p_{\text{model}}(\vx))-\log(p_{\text{noise}}(\vx))).
\end{align*}
\end{gather*}

Now, the estimate \(p_{\text{joint}}(\vx)\) is bayes optimal if
\[
e^c\tilde{p}_{\text{model}}(\vx;\theta) = p_{\text{data}}(\vx)
\]
Hence, optimizing the upper loss will yield us the right \(c\) and \(\theta\).
\sep

\Thm The estimator for \(\theta\) is consistent.

\Thm The estimator for \(\theta\) is generally not statistically efficient.

\sep

\todo{Say why NCE is not the final answer}

\subsection{Variational Autoencoders (VAEs)}

\textbf{Relation to Autoencoders}

Recall, that with autoencoders, we had defined a concatenation of two
differentiable (non-linear) mappings
\(\vx\stackrel{E}{\mapsto}\vz\stackrel{D}{\mapsto}\vhx\) (an encoder \(E\) and a
decoder \(D\)) and trained it with the following loss \(\norm{\vx - \vhx}_2^2\)
(approximating identity function) in order to learn some compressed
representation \(\vz\) of \(\vx\) which just contains the essence of \(\vx\) according
to some meaningful feature-dimensions.

Further, we could then use \(E\) to bootstrap a classification model, by first
applying \(E\), and then a classification network \(C\) and fine-tuning them jointly
on the cross-entropy loss.

So the lower-dimensional features \(\vz\) capture the factors of variation in
the data. And we can reconstruct an \(\vx\) from its compressed representation
\(\vx\).

Now the question is can we us a similar kind of setup to use new images?

\sep

VAEs define an \emph{intractable} density function \(p_{\text{model}}(\vx)\) with
a latent \(\vz\). Having this latent variable allows us to build a network
similar to an autoencoder. A tractable lower bound for the intractable density
function is then derived and optimized.

\(
p_{\text{model}}(\vx): =
p_\theta(\vx) = \int_{\cZ}
\underbrace{p_\theta(\vz)}_{\mathclap{\substack{
\text{``latent representation''}\\
\text{sample from prior}\\
\text{(simple, e.g., MV - Gaussian)}}}}
\overbrace{\cDist{p_\theta}{\vx}{\vz}}^{\mathclap{\substack{
\text{``Decoder''}\\
\text{sample from conditional}\\
\text{(complex NN)}}}} \,d\vz 
\)

With VAEs we assume that our data is generated from some underlying unobserved
representation \(\vz\). We first sample \(\vz\) and then generate some \(\vx\) from
the conditional distribution. Now, we just have to learn the parameters \(\theta\)
that maximize the likelihood of the training data. Unfortunately, we cannot
optimize the likelihood of our model for the data directly (as it's
intractable). 

\(\mcr{\int_{\cZ}} \mcg{p_\theta(\vz)}\mcg{\cDist{p_\theta}{\vx}{\vz}} \,d\vz\)
\quad\tcr{intractable!}, \tcg{tractable}

Note that different factorizations of the distributions would also intractable

\(\cDist{p_\theta}{\vz}{\vx} = \frac{\tcg{\cDist{p_\theta}{\vx}{\vz}
\Dist{p_\theta}{\vz}}}{\tcr{\Dist{p_\theta}{\vx}}}\)

Now, the solution to this is that in addition to the decoder network modeling
\(\cDist{p_\theta}{\vx}{\vz}\), we define an additional encoder network
\(\cDist{q_\phi}{\vz}{\vx}\) that approximates \(\cDist{p_\theta}{\vz}{\vx}\).

\(
\cDist{p_\phi}{\vz}{\vx}
\approx
\cDist{p_\theta}{\vz}{\vx} = \frac{\tcg{\cDist{p_\theta}{\vx}{\vz}
\Dist{p_\theta}{\vz}}}{\tcr{\Dist{p_\theta}{\vx}}}
\)

This will allow us to derive a lower bound on the data likelihood that is
tractable, which we can optimize.

\sep

A common way to define these distributions is as follows: We assume the latent
variable to follow a multivariate gaussian (assumed to be a reasonable prior for
latent attributes).

\textbf{Prior:} \(\vz\sim\cN(\vo,\MI)\).

\textbf{Enc. Netw. \(E\):} models \(\cDist{q_\phi}{\vz}{\vx}\) with params
\(\phi\) and maps \(\vx\stackrel{E}{\mapsto}(\vmu_{\vz|\vx},\MSigma_{\vz|\vx})\)

\textbf{Dec. Netw. \(D\):} models \(\cDist{p_\theta}{\vx}{\vz}\) with params
\(\theta\) and maps \(\vz\stackrel{D}{\mapsto}(\vmu_{\vx|\vz},\MSigma_{\vx|\vz})\)

Note that we use both \emph{diagonal} covariance matrices for
\(\MSigma_{\vz|\vx}\) and \(\MSigma_{\vx|\vz}\). So the output of both networks are
just two vectors (one for mean, other for diagonal).

\Com Encoder and decoter netw. also called ``recognition/inference'' and
``generation'' networks.

\sep

Now, equipped with our encoder and decoder networks, we can rewrite the data
(log) likelihood as follows (note that we omit the product for all points -
you'd just have to put a sum over all the instances in front of everything)

\begin{gather*}
\begin{align*}
\log(p_\theta(\vx))
&=\Exp[\vz\sim \cDist{q_\phi}{\vz}{\vx}]{\log(p_\theta(\vx))}
\quad (\log(p_\theta(\vx))\text{ does not depend on \(\vz\)})
\\
&=
\Exp[\vz]{
\log\left(
\frac{
\cDist{p_\theta}{\vx}{\vz}\Dist{p_\theta}{\vz}
}{
\cDist{p_\theta}{\vz}{\vx}
}
\right)
}
\quad(\text{Bayes Rule})
\\
&=
\Exp[\vz]{
\log\left(
\frac{
\cDist{p_\theta}{\vx}{\vz}\Dist{p_\theta}{\vz}
}{
\cDist{p_\theta}{\vz}{\vx}
}
\frac{\cDist{q_\phi}{\vz}{\vx}}{\cDist{q_\phi}{\vz}{\vx}}
\right)
}
\quad(\text{Muliply by 1})
\\
&=
\Exp[\vz]{\log\left(\cDist{p_\theta}{\vx}{\vz}\right)}
-
\Exp[\vz]{\log\left(
\frac{
\cDist{q_\phi}{\vz}{\vx}
}{
\Dist{p_\theta}{\vz}
}
\right)}
+
\Exp[\vz]{\log\left(
\frac{
\cDist{q_\phi}{\vz}{\vx}
}{
\cDist{p_\theta}{\vz}{\vx}
}
\right)}
\\
&=
\underbrace{
\mcg{\underbrace{\Exp[\vz]{\log\left(\cDist{p_\theta}{\vx}{\vz}\right)}}_{(1)}
-
\underbrace{KL(\cDist{q_\phi}{\vz}{\vx},\Dist{p_\theta}{\vz})}_{(2)}}
}_{\cL(\vx,\theta,\phi)}
+
\mcr{
\underbrace{KL(\cDist{q_\phi}{\vz}{\vx},\cDist{p_\theta}{\vz}{\vx})}_{(3)\geq 0}
}
\end{align*}
\end{gather*}

\begin{enumerate}[label=(\arabic*)]
  \item Decoder network gives us \(\cDist{p_\theta}{\vx}{\vz}\) and we can compute
  estimates of this term through sampling. (Sampling differentiable through
  \emph{reparametrization trick!})\\
  This term ensures that we reconstruct the data well.
  \item This KL term (between Gaussians for encoder and \(\vz\) prior) has a nice
  closed-form solution.\\
  \todo{Write down what it is!}\\
  This term ensures that the approximate posterior distribution is close to
  prior.
  \item \(\cDist{p_\theta}{\vz}{\vx}\) is intractable (as seen earlier). But we
  know that the KL-divergence is \(\geq 0\).
\end{enumerate}

Now what we have is a \emph{tractable lower bound \(\cL\)} (so-called
\tcb{variational lower bound, or evidence lower bound ``ELBO''}) for the
likelihood

\(
\cL(\vx,\theta,\phi)\leq \log(p_\theta(\vx))
\)

and we can take its gradient to optimize:

\(
(\theta^*,\phi^*) = \argmax_{(\theta,\phi)}\sum_{i = 1}^n\cL(\vx^{(i)},\theta,\phi).
\)

\sep

\textbf{Reparametrization Trick}

\todo{Write it down here!!}

\sep

\textbf{Forward Pass and Backpropagation}

\begin{center}
  \includegraphics[width=1\linewidth]{%
img/VAE-forward-prop}
\end{center}

So, first we do the whole backpropagation. And then we just compute the updates
to the parameters \(\theta\) and \(\phi\) via backpropagation.

\todo{Show exactly what the losses are (1) and (2) (or do it as an exercise)}

\sep

\textbf{Generating Data} Here we just sample from the prior, and pass it through
the decoder network to get the posterior distribution's parameters, then we
sample from that one.

\begin{center}
  \includegraphics[width=0.5\linewidth]{%
img/VAE-generation}
\end{center}

\subsection{Deep Latent Gaussian Models}






\subsection{Generative Adversarial Networks (GANs)}

GANs do not try to model a density function but directly aim to build a function
to generated data (implicit generative method). The whole optimization motivated
by a game-theoretic approach in a 2-player game. GANs sample from a simple
random noise distribution and try to learn a transformation (via a NN) to a data
distribution.

\sep

\Def[Discriminator \(D\)] must be a differentiable function parametrized by
\(\theta_d\)

\(
\vx\stackrel{D}{\mapsto}\Prob{\vx \text{ comes  from true data distr.}}\in[0,1]
\)

\ssep

\Def[Generator \(G\)] must be a differentiable function parametrized by \(\theta_g\)

\(
\vz\sim\cN(\vmu,\MSigma)
\stackrel{G}{\mapsto}
\vx
\text{ (one falsified data sample)}
\) (one may also use another prior)

\(\vz\) is sampled from the prior distr. over latent vars (source of randomness)

\sep

\(D\) tries to make \(D(G(\vz))\) near 0 (for fake data)

\(D\) tries to make \(D(\vx)\) near 1 (\(\vx\) sampled from true data)

\(G\) tries to make \(D(G(\vz))\) near 1

\Com In some sense \(G\) implicitly tries to make \(D(\vx)\) near 0 (since it uses
the negative loss of \(D\)) see Minimax game VS Non-Saturating game.

\sep

\textbf{Minimax Game:} Both \(D\) and \(G\) try to minimize and maximize the same
value function:

\begin{gather*}
\begin{align*}
V(G,D)
&=\min_{G}\max_{D} \cR^{(D)}
\\
&=\min_{\theta_g}\max_{\theta_d}
\Exp[\vx\sim p_{\text{data}}]{\log(D_{\theta_d}(\vx))}
+
\Exp[\vz\sim p(\vz)]{\log(1-D_{\theta_d}(G_{\theta_g}(\vz)))}
\end{align*}
\end{gather*}

\todo{Write it in terms of cross-entropy!!}

\(
\cR^{(D)}
= 
-\frac{1}{2}
\Exp[\vx\sim p_{\text{data}}]{\log(D(\vx))}
- \frac{1}{2}
\underbrace{\Exp[\vz\sim \cN(\vmu,\MSigma)]{\log(1 - D(G(\vz)))}}_{=
\Exp[\vx\sim p_{\text{generator}}]{\log(1 - D(\vx))}}
\)

\(
\cR^{(G)}
= 
-\cR^{(D)}
\) 

\begin{itemize}
  \item The loss \(\cR^{(D)}\) is simply the cross-entropy between \(D\)'s
  predictions and the correct labels in the binary classification task (real/fake)
  \item The equilibrium of this game is saddle point of the discriminator loss
  \item If we look for this equilibrium the whole procedure resembles
  minimizing the Jensen-Shannon divergence between the true data distribution
  and the generator distribution.
  \item So \(G\) minimizes the log-probability of \(D\) being correct
\end{itemize}

\sep

\todo{Show Equivalence to Jensen Shannon Divergence}

\sep

\textbf{What is the solution \(D(\vx)\) in terms of \(p_{\text{data}}\) and
\(p_{\text{generator}}\) at the equilibrium?}

In the equilibrium it must hold that the gradient of the discriminator is zero,
because the discriminator otherwise would improve (thus change) itself.

\(
\frac{\partial \cR^{(D)}}{\partial D(\vx)}
= 
-\frac{1}{2}
\Exp[\vx\sim p_{\text{data}}]{\frac{1}{D(\vx)}}
+ \frac{1}{2}
\Exp[\vx\sim p_{\text{generator}}]{\frac{1}{1 - D(\vx)}}
\mbeq 0
\)

\(
\Longleftrightarrow
\Exp[\vx\sim p_{\text{data}}]{\frac{1}{D(\vx)}}
= 
\Exp[\vx\sim p_{\text{generator}}]{\frac{1}{1 - D(\vx)}}
\)

\(
\Longleftrightarrow
\int_{\cX} p_{\text{data}}(\vx)\frac{1}{D(\vx)}\,d\vx
= 
\int_{\cX} p_{\text{generator}}(\vx)\frac{1}{1 - D(\vx)}\,d\vx
\)

Recall: we can get rid of the integral as it is just a function operator. Using
the inverse of the operator, we can get rid of it and the solution constraints
on the optimal \(D(\vx)\) still remain the same.

\(
\Longleftrightarrow
p_{\text{data}}(\vx)\frac{1}{D(\vx)}
= 
p_{\text{generator}}(\vx)\frac{1}{1 - D(\vx)}
\)

\begin{comment}
\(
\Longleftrightarrow
p_{\text{data}}(\vx)\frac{1 - D(\vx)}{D(\vx)}
= 
p_{\text{generator}}(\vx)
\)

\(
\Longleftrightarrow
p_{\text{data}}(\vx)\frac{1}{D(\vx)} -  p_{\text{data}}(\vx)
= 
p_{\text{generator}}(\vx)
\)

\(
\Longleftrightarrow
p_{\text{data}}(\vx)\frac{1}{D(\vx)}
= 
p_{\text{data}}(\vx) +  p_{\text{generator}}(\vx) 
\)

\(
\Longleftrightarrow
\frac{1}{D(\vx)}
= 
\frac{p_{\text{data}}(\vx) + p_{\text{generator}}(\vx)}{p_{\text{data}}(\vx)}
\)

\(
\Longleftrightarrow
D(\vx)
= 
\frac{p_{\text{data}}(\vx)}{p_{\text{data}}(\vx) + 
p_{\text{generator}}(\vx)} \)
\end{comment}

Then we get the following stationarity
condition: The optimal \(D(\vx)\) for any \(p_{\text{data}}(\vx)\) and
\(p_{\text{generator}}(\vx)\) is always

\(
D(\vx)
= 
\frac{p_{\text{data}}(\vx)}{p_{\text{data}}(\vx) +  p_{\text{generator}}(\vx)}
\)

Note that by stationarity condition we mean that this must hold in the
Nash equilibrium (where both players stop adapting themselves).

Estimating this ratio using supervised learning is the key approximation
mechanism used by GANs.

\todo{Maybe relate this to other ratios where \(\alpha\neq 1\). Implicit Models}

\todo{Why did we want to show this??}

What assumptions are needed to obtain this solution?

We need to assume that both densities are nonzero everywhere. If we don't make
this assumption then there's this issue that the discriminator's input space
might never be sampled during its training process. Then these points would have
an undefined behaviour since they're never trained.

\sep

\textbf{Training Procedure:} Use SGD-like algorithm of choice (ADAM) on two
minibatches simultaneously. At each iteration, we choose:
\begin{itemize}
  \item a minibatch of true data samples
  \item a minibatch of noise vectors to produce minibatch generated samples
\end{itemize}
Then compute both losses and perform gradient updates.

\(\theta_d^{t + 1}\gets\theta_d^t - \eta_t\nabla_{\theta_d}\cR^{(D)}(\theta_d)\)

\(\theta_g^{t + 1}\gets\theta_g^t - \eta_t\nabla_{\theta_g}\cR^{(G)}(\theta_g)\)

Optional: run \(k\geq 1\) update steps of \(D\) for every iteration (and only
\(1\) update step for \(G\)).

So we alternate between
\begin{itemize}
  \item \textbf{Gradient ascent} on \(D\)\\
  \(
\max_{\theta_d}
\Exp[\vx\sim p_{\text{data}}]{\log(D_{\theta_d}(\vx))}
+ 
\Exp[\vz\sim p(\vz)]{\log(1 - D_{\theta_d}(G_{\theta_g}(\vz)))}
  \)
  \item \textbf{Gradient descent} on \(G\)\\
  \(
\min_{\theta_g}
\Exp[\vz\sim p(\vz)]{\log(1 - D_{\theta_d}(G_{\theta_g}(\vz)))}
  \)
\end{itemize}

\ssep

Note that this training algorithm uses a heuristically motivated loss (that is
a bit different) for the generator to have better gradients when the
discriminator is good:

\begin{center}
  \includegraphics[width=1\linewidth]{%
img/GAN-training}
\end{center}

\sep

\todo{From here on until end was not treated in lecture. See comments}

\todo{Include Perceptron and it's learning algorithm}

\begin{comment}
What prevents the generator from just learning about one specific true image and
then constantly outputting it? (as it would be indistinguishable)

If we're able to correctly play the Minimax game then \(G\) is not able to
consistently fool \(D\) by always generating the same sample. \(D\) would learn to
recognize that sample and reject it as fake.

\todo{Also write about mode collapse.}

\sep

Improvements of this model could use importance sampling in both latent and true
data space to improve \(G\).

\sep

\textbf{Non-Saturating Game}

Here we change \(G\)'s loss as follows

\(
\cR^{(G)}
= 
-\frac{1}{2}\Exp[\vz\sim\cN(\vmu,\MSigma)]{D(G(\vz))}.
\)

Hence the equilibrium is no longer describable with a single loss. Now the
generator just maximizes the log-probability of the discriminator being
mistaken. Heuristically motivated: \(G\) can still learn even when
\(D\) successfully rejects all generator samples.

The problem with the Minimax game is that when \(D\) becomes to smart, the
gradient for \(G\) goes away.

\sep
\end{comment}

