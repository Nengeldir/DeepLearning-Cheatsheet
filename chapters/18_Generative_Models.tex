\section{Generative Models}

\subsection{Variational Autoencoders (VAEs)}
Latent variable models $p(x,z) = p(x|z)p(z)$ where the posterior $p(z|x)$ is intractable. VAEs approximate it using a parametric encoder $q_\phi(z|x)$.

\Def[Evidence Lower Bound (ELBO)]:
Since $\ln p(x)$ is intractable, we maximize a lower bound (Jensen's Inequality):

\begin{align*}
\ln p_\theta(x) &\ge \mathcal{L}(\theta, \phi; x) = \\
&\underbrace{\mathbb{E}_{q_\phi(z|x)}[\ln p_\theta(x|z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{Regularization}} 
\end{align*}

\begin{itemize}
    \item \textbf{Encoder ($q_\phi$)}: Maps input $x$ to latent parameters $\mu, \Sigma$.
    \item \textbf{Decoder ($p_\theta$)}: Reconstructs $x$ from sampled $z$.
\end{itemize}

\sep

\Def[Reparameterization Trick]:
To backpropagate through the stochastic node $z \sim \mathcal{N}(\mu, \Sigma)$, we move the noise outside:
$$ z = \mu + \Sigma^{1/2} \odot \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) $$
This makes $z$ a deterministic, differentiable function of $\phi$ and fixed noise $\epsilon$.

\subsection{Factor Analysis}
defines a proper probabilistic model of the data ($m \ll n$):\\
\begin{itemize}
    \item choose a probability density function $p_Z$ over the latents
    \item define a conditional probability density function $p_{X|Z}$ over observables
    \item integrate out the latent variables
        \[
        p_X(x) \;=\; \int p_Z(z)\,p_{X\mid Z}(x\mid z)\,dz
        \]
    \item use the Gaussian prior density $z \sim \mathcal{N}(0, I), \quad z \in \mathbb{R}^m$
    \item add linear observation model for $x \in \mathbb{R}^n$
        \[
        x = \mu + W z + \eta,\quad
        \eta \sim \mathcal{N}(0,\Sigma),\quad
        \Sigma=\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2)
        \]
        \[
        x\in\mathbb{R}^n,\quad z\in\mathbb{R}^m,\quad W\in\mathbb{R}^{n\times m}
        \]
    \item observational noise $\eta$ is independent of the latents $z$
    \item The induced density is itself normal
        \[
        x \sim \mathcal{N}\!\big(\mu,\; W W^\top + \Sigma\big), \quad
        \mu \;=\; \frac{1}{s}\sum_{i=1}^s x_i
        \]
    \item factors are only identifiable up to orthogonal transformations (rotations, reflections, or permutations) in ${\mathbb{R}}^m$, 
        because for any orthogonal matrix $Q \in \mathbb{R}^{m \times m}$ holds:
        \[
        (WQ)(WQ)^\top = W Q Q^\top W^\top = W W^\top \quad\text{if } Q Q^\top = I
        \]
    \item posterior is normal with mean and covariance matrix
        \[
        \mu_{z\mid x} \;=\; W^\top (W W^\top + \Sigma)^{-1} (x - \mu)
        \]
        \[
        \Sigma_{z\mid x} \;=\; I - W^\top (W W^\top + \Sigma)^{-1} W.
        \]
    \item For vanishing isotropic noise (probabilistic PCA) ($\Sigma = \sigma^2 I, \sigma^2 \to 0$), the pseudo-inverse of $W$ converges (if $W$ has orthogonal colums $W^{+} = W^\top$)
        \[
        W^\top (W W^\top + \sigma^2 I)^{-1} \;\longrightarrow\; W^{+} \in \mathbb{R}^{m\times n}
        \]
    \item Given W, it is thus easy to calc. the posterior over z.
        \[
        \mu_{z\mid x} \;\longrightarrow\; W^{+}(x-\mu),\quad
        \Sigma_{z\mid x} \;\longrightarrow\; 0
        \]
    \item MLE with a sample set $\mathcal{S}$: \(\mu, W \leftarrow \log p(\mu, W)(S)\)
    \item The optimality condition of the $i$-th column of $W$ is
        \[
        w_i = \rho_i\,u_i,\qquad
        \rho_i = \max\{0,\;\sqrt{\lambda_i - \sigma^2}\}.
        \]
        where $u_i$ is the $i$-th principal eigenvector of the sample covariance matrix and $\lambda_i$ the corresponding eigenvalue.
    \item For $\sigma^2 \to 0$, we recover PCA
\end{itemize}

\sep

\subsection{Normalizing Flows}
Learns a bijective mapping $f: \mathcal{Z} \to \mathcal{X}$ from a simple distribution $p_z$ (e.g., Gaussian) to the complex data distribution $p_x$. Allows \textbf{exact likelihood} computation.

\sep

\Def{Change of Variables}:
$$ p_x(x) = p_z(z) \left| \det \frac{\partial f^{-1}(x)}{\partial x} \right| = p_z(f^{-1}(x)) |\det J_{f^{-1}}(x)| $$
Or in log-domain (maximizing likelihood):
$$ \ln p_x(x) = \ln p_z(z) - \ln \left| \det \frac{\partial f(z)}{\partial z} \right| $$

\sep

\Def{Coupling Layers (RealNVP)}:
To ensure the Jacobian determinant is computationally cheap, we split variables $x_{1:d}$ and $x_{d+1:D}$:
\begin{align*}
    y_{1:d} &= x_{1:d} \\
    y_{d+1:D} &= x_{d+1:D} \odot \exp(s(x_{1:d})) + t(x_{1:d})
\end{align*}
The Jacobian is triangular, so $\det J = \prod \exp(s(x_{1:d}))$.

\sep

\subsection{Gen. Adversarial Networks (GANs)}
A minimax game between a Generator $G$ (creates fakes) and Discriminator $D$ (classifies real vs. fake).

\Def[Minimax Objective]:
\begin{align*}
    \min_G \max_D V(D, G) = \\
    \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
\end{align*}

\textbf{Optimality}:
\begin{itemize}
    \item \textbf{Optimal Discriminator}: For a fixed $G$, $D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$.
    \item \textbf{Global Minimum}: Achieved when $p_g = p_{data}$. The value is $-\log 4$ (related to Jensen-Shannon Divergence).
\end{itemize}

\sep

\Com \textbf{Training Issues}:
\begin{itemize}
    \item \textbf{Vanishing Gradients}: If $D$ is perfect, $\log(1-D(G(z)))$ saturates. Fix: Train $G$ to maximize $\log D(G(z))$ (Non-Saturating Loss).
    \item \textbf{Mode Collapse}: $G$ maps all $z$ to a single plausible $x$ to cheat $D$.
\end{itemize}

\sep

\subsection{Denoising Diffusion Models (DDPM)}
Learns to reverse a gradual noising process.

\Def[Forward Process (Fixed)]:
Markov chain adding Gaussian noise according to schedule $\beta_t$:
$$ q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$
\textbf{Closed form} sampling at step $t$ (using $\alpha_t = 1-\beta_t$ and $\bar{\alpha}_t = \prod \alpha_i$):
$$ x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, I) $$

\sep

\Def[Reverse Process (Learned)]:
Approximated by a neural network with parameters $\theta$:
$$ p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$

\sep

\Def[Simplified Objective]:
Instead of predicting the image mean $\mu$, we predict the noise $\epsilon$ added at step $t$:
$$ L_{simple} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t) \|^2 \right] $$

\subsection{Autoregressive Models}
\Thm[AR for Density Estimation]\\
Any joint density can be factorized as a product of conditionals
\[
p(\mathbf{x}) \;=\; \prod_{i=1}^d p(x_i \mid \mathbf{x}_{<i})
\]
Traditional linear AR models in statistics use:
\[
p(x_i \mid \mathbf{x}_{<i}) \;=\; \mathcal{N}\!\left(x_i;\; \sum_{k=1}^K \theta_k x_{i-k},\; \sigma^2\right)
\]
To obtain richer models, we can model each conditional with a DNN
Properties:
\begin{itemize}
    \item exact likelihood evaluation (fully tractable)
    \item sampling is sequential over dimensions (can be slow)
    \item AR structure $\to$ triangular Jacobian in flows
\end{itemize}

VAEs \& GANs are complicated to learn / not always successful. AR models are simple (look at chainrule \(p(x_1,\dots,x_m) \;=\; \prod_{t=1}^m p(x_t \mid x_{1:t-1})\))