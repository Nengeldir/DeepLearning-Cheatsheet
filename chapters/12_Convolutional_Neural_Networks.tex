\section{Convolutional Neural Networks}

\subsection{Convolutional Layers}

\Def[Transform] A transform \(T\) is a mapping from one function space \(\cF\) to another function space \(\cF'\). So \(T\colon\cF\to\cF'\).

\sep

\Def[Linear Transform] A transform \(T\) is linear, if for all functions \(f,g\) and scalars \(\alpha,\beta\), \(T(\alpha f + \beta g) = \alpha (T f) + \beta (T g)\).

\sep

\Def[Integral Transform] An \emph{integral transform} is any transform \(T\) of the following form

\[(T f)(u) = \int_{t_1}^{t_2} K(t,u) f(t) \, dt.\]

\Com The fourier transform is an example of an integral transform.

\sep

\Thm Any integral transform is a linear transform.

\sep

\Def[Convolution] Given two functions \(f, h\colon\R\to\R\), their convolution is defined as

\[(f * h)(u) := \int_{-\infty}^{\infty} h(t)f(u - t) \, dt = \int_{-\infty}^{\infty} h(u - t)f(t)  \, dt \]

\Com Whether the convolution exists depends on the properties of \(f\) and \(h\) (the integral might diverge). However, a typical use is \(f = \text{signal}\), and \(h = \text{fast decaying kernel function}\).

\sep

\Thm[Convolution Theorem] Any linear, translation-invariant transformation \(T\) can be written as a \emph{convolution} with a suitable \(h\).

\sep

\Thm[Convs are commutative and associative]

\sep
\Thm[Convs are shift-invariant], we define \(f_{\Delta}(t) := f(t + \Delta)\). Then 

\[(f_{\Delta} * h)(u) = (f * h)_{\Delta}(u)\]

\sep

\Def[Fourier Transform] The fourier transform of a function \(f\) is defined as

\[(\mathcal{F} f)(u) := \int_{-\infty}^{\infty} f(t) e^{-2\pi i u t} \, dt\]
and its inverse as
\[(\mathcal{F}^{-1} f)(u) := \int_{-\infty}^{\infty} f(t) e^{2\pi i u t} \, dt\]

\Com Convolutional operators can be efficiently computed with point wise multiplication using the Fourier transform.

\[\mathcal{F}(f * h) = \mathcal{F} f \cdot \mathcal{F} h\]
and then transformed back using the inverse Fourier transform.

\[\mathcal{F}^{-1}(\mathcal{F}(f * h)) = \mathcal{F}^{-1}(\mathcal{F} f \cdot \mathcal{F} h) = f * h\]

\subsection{Discrete Time Convolutions}

\Def[Discrete Convolution]

For \(f,h\colon\Z\to\R\), we can define the discrete convolution via

\[(f*h)[u] := \sum_{t = -\infty}^\infty f[t]h[u - t] = \sum_{t = -\infty}^\infty f[u - t]h[t]\]

\Com Note that the use of rectangular brackets suggests that we're using ``arrays'' (discrete-time samples).

\Com Typically we use a \(h\) with finite support (window size).

\sep

\Def[Multidimensional Discrete Convolution] 

For \(f,h\colon \R^d\to\R\) we have
\begin{align*}
    (f*h)[u_1,\ldots,u_d] = \\
    \sum_{t_1=-\infty}^\infty \cdots \sum_{t_d=-\infty}^\infty f(t_1,\ldots,t_d)h(u_1-t_1,\ldots,u_d-t_d) = \\
    \sum_{t_1=-\infty}^\infty \cdots \sum_{t_d=-\infty}^\infty f(u_1-t_1,\ldots,u_d-t_d)h(t_1,\ldots,t_d)
\end{align*}

\sep

\Def[Discrete Cross-Correlation]

Let \(f,h\colon\Z\to\R\), then
\begin{align*}
    (h\star f)[u]:= \\
    & \sum_{t=-\infty}^\infty h[t]f[u+t] = \sum_{t=-\infty}^\infty h[-t]f[u-t] \\
    & (\overline{h}\star f)[u] =(f\star\overline{h})[u] \quad \text{where }\overline{h}(t)=h(-t).
\end{align*}

aka ``sliding inner product'', non-commutative, kernel ``flipped over'' (\(u + t\) instead of \(u - t\)). If kernel symmetric: cross-correlation = convolution.

\subsection{Convolution via Matrices}

Represent the input signal, the kernel and the output as \emph{vectors}. Copy the kernel as columns into the matrix ofsetting it by one more very time (gives a band matrix (special case of Toeplitz matrix)). Then the convolution is just a matrix-vector product.

\subsection{Border Handling}

There are different options to do this
\begin{itemize}
  \item \Def[Padding of \(p\)] Means we extend the image (or each dimension) by \(p\) on both sides (so +2\(p\)) and just fill in a constant there (e.g., zero).
  \item \Def[Same Padding] Padding with zeros = \emph{same padding} (``same'' constant, i.e., 0, and we'll get a tensor of the ``same'' dimensions)
  \item \Def[Valid Padding] Only retain values from windows that are fully-contained within the support of the signal \(f\) (see 2D example below) = \emph{valid padding}
\end{itemize}

\subsection{Backpropagation for Convolutions}

\Def[Receptive Field \(\cI_i^l\) of \(x_i^l\)] 

The \emph{receptive field} \(\cI_i^l\) of node \(x_i^l\) is defined as \( \cI_i^l := \dset{j}{W^l_{ij}\neq 0} \) where \(\MW^l\) is the Toeplitz matrix of the convolution at layer \(l\).

\Com Hence, the receptive field of a node \(x_i^l\) are just nodes the which are connected to it and have a non-zero weight.

\Com One may extend the definition of the receptive field over several layers. The further we go back in layer, the bigger the receptive field becomes due to the nested convolutions. The receptive field may be even the entire image
after a few layers. Hence, the convolutions have to be small.

\sep

We have \(\forall j\neq \cI_i^l\colon\frac{\partial x_i^l}{\partial x_j^{l - 1}} = 0,\)

\sep

Due to \emph{weight-sharing}, the kernel weight \(h_j^l\) is re-used for every unit in the target layer at layer \(l\), so when computing the derivative \(\frac{\partial\cR}{\partial h_j^l}\) we just build an additive combination of all the derivatives (note that some of them might be zero).

\[\frac{\partial \cR}{\partial h_j^l} = \sum_{i = 1}^{m_l}\frac{\partial \cR}{\partial x_i^l}\frac{\partial x_i^l}{\partial h_j^l}\]

\sep

\textbf{Backpropagations of Convolutions as Convolutions}

\(\vy^{(l)}\) output of \(l\)-th layer \(\vy^{(l - 1)}\) output of \((l - 1)\)-th layer / input to \(l\)-th layer \(\vw\) convolution filter \(\frac{\partial\cR}{\partial \vy^{(l)}}\) known \(\vy^{(l + 1)} = \vy^{(l)}\conv \vw\)

\begin{align*}
\frac{\partial\cR}{\partial w_i} &= \sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k}\frac{\partial y^{(l)}_k}{\partial w_i} = \sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k}\frac{\partial}{\partial w_i} \left[\vy^{(l)}\conv\vw\right]_k \\
&=\sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k}\frac{\partial}{\partial w_i} \left[\sum_{o=-p}^{p} y^{(l-1)}_{k-o}w_o\right] = \sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k} y^{(l-1)}_{k-i} \\
&=\sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k} y^{(l-1)}_{-(k-i)} = \sum_{k}\frac{\partial\cR}{\partial y^{(l)}_k} \text{rot180}(y^{(l-1)})_{k-i} \\
&=\left(\frac{\partial\cR}{\partial \vy^{(l)}} \conv\text{rot180}(y^{(l-1)})\right)_i
\end{align*}

The derivative \(\frac{\partial\cR}{\partial\vy^{(l)}}\) is analogous. 

Note that we just used generalized indices \(i,k,o\) which may be multi-dimensional.

This example omits activation functions and biases, but that could be easily included with the chain-rule.

\sep

\Def[Rotation180] \(\forall i\colon\text{rot180}(\vx)_i = \vx_{(-i)}.\)


\subsection{Pooling}

There are min, max, avg, and softmax pooling. Max pooling is the most frequently used one.

\sep

\Def[Max-Pooling] 

\begin{itemize}
  \item 1D: \(x_i^{\max} = \max \dset{x_{i + k}}{0\leq k < r}\)
  \item 2D: \(x_{ij}^{\max} = \max \dset{x_{i + k,j + l}}{0\leq k,l < r}\)
\end{itemize}

\subsection{Sub-Sampling (aka ``Strides'')}

Often, it is desirable to reduce the size of the feature maps. This can be achieved by skipping some of the input values in the convolution. The stride is the number of steps the kernel takes in each direction.

\subsection{Channels}

\Ex Here we have
\begin{itemize}
  \item an input signal that is 2D with 3 channels (7x7x3) (image x channels)
  \item and we want to learn two filters \(W0\) and \(W1\), which each process the 3 channels, and sum the results of the convolutions across each channel leading to a tensor of size 3x3x2 (convolution result x num convolutions)
\end{itemize}

\begin{center}
  \includegraphics[width=0.5\linewidth]{img/convolutions-with-channels}
\end{center}

Usually we convolve over all of the channels together, such that each convolution has the information of all channels at its disposition and the order of the channels hence doesn't matter.

%Check that
\subsection{CNNs in Computer Vision}

So the typical use of convolution that we have in vision is: a sequence of convolutions

\begin{enumerate}
  \item that \emph{reduce} the spatial dimensions (sub-sampling)
  \item that \emph{increase} the number of channels
\end{enumerate}

The deeper we go in the network, we transform the spatial information into a semantic representation. Usually, most of the parameters lie in the fully connected layers

\subsubsection{Classic CNN Architectures}

\Def[LeNet-5 (1998)] The pioneering CNN for handwritten digit recognition (MNIST).
\begin{itemize}
    \item \textbf{Structure}: 2 Convolutional layers (with Average Pooling) followed by 3 Fully Connected layers.
    \item \textbf{Key Features}: Introduced the concepts of local receptive fields, shared weights, and spatial subsampling. Used Sigmoid/Tanh activations (pre-ReLU).
\end{itemize}

\sep

\Def[AlexNet (2012)] The breakthrough model that popularized Deep Learning on ImageNet.
\begin{itemize}
    \item \textbf{Structure}: Deeper than LeNet (5 Conv layers, 3 FC layers). Used large filters initially ($11 \times 11$).
    \item \textbf{Innovations}: First large-scale use of \textbf{ReLU} (to solve vanishing gradients), \textbf{Dropout} (for regularization), and \textbf{Data Augmentation}. Trained on GPUs.
\end{itemize}

\sep

\Def[VGG Network (2014)]
Focused on the effect of network depth using a uniform architecture.
\begin{itemize}
    \item \textbf{Philosophy}: Replace large filters (e.g., $5 \times 5$, $7 \times 7$) with stacks of small \textbf{$3 \times 3$ filters}.
    \item \textbf{Reasoning}: Two stacked $3 \times 3$ layers have the same receptive field as a $5 \times 5$ layer but with fewer parameters and more non-linearities (ReLU between layers).
\end{itemize}

\sep

\Def[Inception Network]
Focused on computational efficiency and network "width". It was developed by Google in 2014.
\begin{itemize}
    \item \textbf{Inception Module}: Instead of choosing a filter size, it performs $1 \times 1$, $3 \times 3$, and $5 \times 5$ convolutions (and pooling) \textit{in parallel} and concatenates the outputs.
    \item \textbf{$1 \times 1$ Convolutions}: Used as "Bottleneck layers" to reduce dimensionality (depth) before expensive operations, significantly reducing computational cost.
\end{itemize}

\sep

\Def[U-Net (2015)]
Designed for Biomedical Image \textbf{Segmentation} (pixel-wise classification).
\begin{itemize}
    \item \textbf{Structure}: Symmetrical Encoder-Decoder architecture (U-shape).
    \item \textbf{Encoder}: Contracting path (Convs + Max Pooling) to capture context.
    \item \textbf{Decoder}: Expansive path (Up-Convs) to enable precise localization.
    \item \textbf{Skip Connections}: Concatenates high-resolution features from the encoder directly to the decoder to recover spatial details lost during downsampling.
\end{itemize}

\subsubsection{Convolutions in Sequences, NLP \& Audio}

\Def[1D Convolutions] Unlike 2D CNNs for images, sequence modeling (Temporal ConvNets) uses 1D filters that slide over the time axis.
\begin{itemize}
    \item \textbf{Input}: Tensor of shape $(Batch, Length, Channels)$. In NLP, "Channels" are the dimensions of the Word Embeddings.
    \item \textbf{Function}: Captures local temporal dependencies (like $n$-grams in text) effectively.
    \item \textbf{Advantage}: Highly parallelizable (unlike RNNs which are sequential) and computationally efficient.
\end{itemize}

\sep

\Def[Embeddings \& NLP] CNNs are often applied on top of pre-trained word embeddings (e.g., Word2Vec, GloVe).
\begin{itemize}
    \item A sentence is represented as a matrix (Length $\times$ Embedding Dim).
    \item Filters of different widths (e.g., covering 2, 3, or 4 words) act as feature detectors for phrases or specific linguistic patterns (e.g., "very good", "not bad") regardless of their position in the sentence.
\end{itemize}

\Def[Dilated Convolutions] To handle long sequences without losing resolution (pooling), \textit{dilation} introduces gaps between kernel elements.
\begin{itemize}
    \item \textbf{Receptive Field}: Grows exponentially with the dilation factor $d$ ($1, 2, 4, 8 \dots$), allowing the network to capture long-range dependencies with few layers.
\end{itemize}

\Def[WaveNet (2016)] A deep generative model for raw audio waveforms.
\begin{itemize}
    \item \textbf{Causal Convolutions}: Strict ordering ensures the prediction at time $t$ only depends on samples $x_{<t}$ (cannot see the future).
    \item \textbf{Dilated Causal Convolutions}: Stacks layers with increasing dilation factors. This allows the output neuron to have a receptive field of thousands of timesteps (milliseconds of audio) to generate realistic high-fidelity sound structure.
    \item \textbf{Skip Connections}: Uses residual and parameterized skip connections to speed up convergence and allow training of very deep networks.
\end{itemize}

\subsection{Comparison of \#Parameters (CNNs, FC, LC)}

\Ex input image \(m\times n\times c\) (\(c = \) number of channels)

\(K\) convolution kernels: \(p\times q\)  (valid padding and stride 1)

output dimensions: \((m - p + 1)\times (n - q + 1)\times K\)

\ssep

\#parameters CNN: \(K(pqc + 1)\)

\ssep

\#parameters of fully-conn. NN with same number of outputs as CNN:\\
\(mnc((m - p + 1)(n - q + 1) + 1)K\)

\ssep

\#parameters of locally-conn. NN with same connections as CNN:\\
\(pqc((m - p + 1)(n - q + 1) + 1)K\)
