\section{Analysis}

\textbf{Log-Trick (Identity):}
\( \nabla_\theta \left[p_\theta(\vx)\right] =  p_\theta(\vx)\nabla_\theta \left[\log(p_\theta(\vx))\right] \)

\sep

\Thm[Cauchy-Schwarz]

\(\forall\vu,\vv\in V\colon\scprod{\vu,\vv}\leq\abs{\scprod{\vu,\vv}}\leq\norm{\vu}\norm{\vv}\).

\(\forall\vu,\vv\in V\colon 0\leq\abs{\scprod{\vu,\vv}}\leq\norm{\vu}\norm{\vv}\).

Special case: \((\sum x_i y_i)^2\leq(\sum x_i^2)(\sum y_i^2)\).

Special case: \(\Exp{XY}^2\leq\Exp{X^2}\Exp{Y^2}\).

\sep

\Def[Convex Set] A set \(S\subseteq\R^d\) is called \emph{convex} if

\( \forall\vx,\vx'\in\S, \,\,\forall\lambda\in[0,1]\colon \quad \lambda\vx + (1 - \lambda)\vx'\in S. \)

\Com Any point on the line between two points is within the set. \(\R^d\) is convex.

\sep

\Def[Convex Function] A function \(f\colon S\to\R\) defined on a \emph{convex set} \(S\subseteq\R^d\) is called \emph{convex} if

\[\forall\vx,\vx'\in S, \,\lambda\in[0,1]\colon\quad f(\lambda\vx +  (1 - \lambda)\vx')\leq\lambda f(\vx) +  (1 - \lambda)f(\vx)\]

\Com A function is strictly convex if the line segment between any two points on the graph of the function lies strictly above the graph. This guarantees that there is a unique global minimum.

\Thm[Properties of Convex Functions]
\begin{itemize}
  \item \(f(y) \geq f(x) + \nabla f(x)^T(y-x)\)
  \item \(f''(x) \geq 0\)
  \item Local minima are global minima, strictly convex functions have a unique global minimum
  \item If \(f,g\) are convex then \(\alpha f + \beta g\) is convex for \(\alpha,\beta\geq 0\)
  \item If \(f,g\) are convex then \(\max(f,g)\) is convex
  \item If \(f\) is convex and \(g\) is convex and non-decreasing then \(g\circ f\) is convex
\end{itemize}

\sep
\Def[Strongly Convex Function] A function $f$ is $\mu$-strongly convex if it curves up at least as much as a quadratic function with curvature $\mu > 0$. For all $x, y$:
$$ f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2} ||y-x||^2 $$
\textbf{Relation to Optimization}:
\begin{itemize}
    \item \textbf{Guarantee}: Ensures a unique global minimum exists.
    \item \textbf{Convergence}: Gradient Descent on strongly convex (and Lipschitz smooth) functions guarantees a \textbf{linear convergence rate} ($O(c^k)$ for some $c < 1$).
    \item \textbf{Condition Number}: The convergence speed depends on the condition number $\kappa = L/\mu$. If $\kappa$ is large (poor conditioning), convergence slows down.
\end{itemize}

\sep

\Def[Condition Number] The condition number $\kappa(A)$ measures the sensitivity of a function's output to small perturbations in the input. For a symmetric positive semi-definite matrix (like the Hessian $H$ of a loss function), it is the ratio of the largest to the smallest eigenvalue:
$$ \kappa(H) = \frac{|\lambda_{\max}|}{|\lambda_{\min}|} \geq 1 $$

\noindent \textbf{Implications for Optimization}:
\begin{itemize}
    \item \textbf{Well-conditioned ($\kappa \approx 1$)}: The contours of the loss function are nearly spherical. Gradient Descent converges quickly and directly toward the minimum.
    \item \textbf{Ill-conditioned ($\kappa \gg 1$)}: The contours form narrow, elongated ellipses (steep valleys). Gradient Descent tends to oscillate ("zigzag") across the narrow valley rather than moving down the slope, leading to very slow convergence.
\end{itemize}

\Com Momentum and Adaptive Learning Rate methods (like Adam) are specifically designed to mitigate the issues caused by high condition numbers.

\sep

\Thm[Taylor-Lagrange Formula]

\(f(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!}(x - x_o)^k +  \int_{x_0}^x \frac{f^{(n + 1)}(x - t)}{n!}\,dt\)

\sep

\Thm[Jensen] \(f\) convex/\tcr{concave}, \(\forall i\colon\lambda_i\geq 0\), \(\sum_{i = 1}^n\lambda_i = 1\)

\( f\left(\sum_{i = 1}^n\lambda_i\vx_i\right) \leq\mcr{/\geq} \sum_{i = 1}^n\lambda_if\left(\vx_i\right) \)

Special case: \(f(\Exp{X})\leq \Exp{f(X)}\).

\sep

\Def[\(L\)-Lipschitz Continous Function] Given two \emph{metric spaces} \((X, d_X)\) and \((Y, d_Y)\), a function \(f\colon X\to Y\) is called \emph{Lipschitz continuous}, if there exists a real constant \(L\in\R^+_0\) (\emph{Lipschitz constant}), such that

\[ \forall\vx_1,\vx_2\in\R^n\colon\quad\norm{f(\vx_1) -  f(\vx_2)}\leq L\cdot\norm{\vx_1 - \vx_2}. \]

\Com If the objective function is $L$-smooth a step size of $\eta = 1/L$ guarantees convergence.
\sep

\Def[Lagrangian Formulation] of \(\arg\max_{x,y}f(x,y)\) s.t. \(g(x,y) =  c\): \(\mathcal{L}(x, y, \gamma) =  f(x,y) -  \gamma ( g(x,y) - c)\)

\sep

\Def[PL Condition] A differentiable function $f(x)$ with global minimum $f^*$ satisfies the $\mu$-Polyak-Lojasiewicz (PL) condition if there exists a constant $\mu > 0$ such that for all $x$:
$$ \frac{1}{2} ||\nabla f(x)||^2 \geq \mu (f(x) - f^*) $$

\noindent \textbf{Significance}:
\begin{itemize}
    \item \textbf{Gradient Dominance}: It implies that the gradient magnitude dominates the suboptimality. If the gradient is small, the function value must be close to the optimal $f^*$.
    \item \textbf{Convergence without Convexity}: The PL condition is weaker than strong convexity (it does not require convexity at all). However, it is sufficient to guarantee a \textbf{linear convergence rate} for Gradient Descent.
    \item \textbf{In Deep Learning}: Over-parameterized neural networks often satisfy the PL condition in the neighborhood of a minimum, explaining fast convergence despite non-convexity.
\end{itemize}

\noindent \textbf{Convergence Rate}:
Gradient Descent with step size $\alpha = 1/L$ (where $L$ is the Lipschitz constant) converges as:
$$ f(x_k) - f^* \leq \left(1 - \frac{\mu}{L}\right)^k (f(x_0) - f^*) $$
