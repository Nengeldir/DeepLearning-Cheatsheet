\section{Activation Functions}
Activation functions are non-linear functions that are applied element-wise to the output of a linear function.

\sep

\Def[Tanh]

\[\tanh(\vx) = \frac{e^{\vx} - e^{-\vx}}{e^{\vx} + e^{-\vx}} \qquad \tanh'(\vx) = 1 - \tanh^2(\vx)\]

\sep

\Def[ReLU]

\[\mathrm{ReLU}(x) = \max(x, 0) \qquad \qquad \mathrm{ReLU}'(x) = \begin{cases}1 & x > 0 \\ 0 & x \leq 0\end{cases}\]

\sep

\Def[Sigmoid/Logistic]

\[\sigma(\vx)_i = \frac{1}{1 + e^{-x_i}} \in(0,1) \qquad \frac{\partial \sigma(\vx)}{\partial \vx} = \sigma(\vx)\odot(1-\sigma(\vx)) \]

\[\sigma^{-1}(y) = \ln\left(\frac{y}{1 - y}\right) \quad \nabla_{\vx}\sigma(\vx) = \MJ_{\sigma}(\vx) = \diag(\sigma(\vx)\odot(1 - \sigma(\vx)))\]

\[\sigma'(x) = \frac{1}{4}\tanh'(\frac{1}{2}x) =  \frac{1}{4}(1 - \tanh^2(\frac{1}{2}x)) \qquad \sigma(-x) = 1 - \sigma(x)\]

\sep

\textbf{Connection between Sigmoid and Tanh} (Equal Representation Strength)

\[\sigma(x) =  \frac{1}{2}\tanh\left(\frac{1}{2}x\right) + \frac{1}{2} \Longleftrightarrow \tanh(x) =  2\sigma(2x) - 1 \]

\sep

\Def[Softmax] \(\softmax(\vx)_i =f(x)_i \)

\[f(x)_i = \frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}} \quad \frac{\partial f(x)_i}{\partial x_j} = \begin{cases} -f(x)_i f(x)_j & i \neq j \\ f(x)_i - f(x)_i f(x)_j & i = j \end{cases}\]

\[\nabla_x f(x) = \MJ_{f}(x) = \diag(f(x)) - f(x)f(x)^T\]
