\section{Information Theory}

\Def[Entropy] Let \(\rvX\) be a random variable distributed according to \(p(\rvX)\). Then the entropy of \(\rvX\)

\( H(\rvX) = \Exp{-\log(\Prob{X})} = -\sum_{\vx\in\cX} p(\vx)\log\left(p(\vx)\right) \geq 0. \)

It describes the expected information content \(I(\rvX)\) of \(\rvX\). 

\sep

\Def[Cross-Entropy] For distributions \(p\) and \(q\) over a given set is \( H(p,q) =  -\sum_{\vx\in\cX}p(\vx)\log(q(\vx)) =  \Exp[\vx\sim p]{-\log(q(\vx))} \geq 0. \)

\( H(X;p,q) =  H(X) +  KL(p,q) \geq 0, \quad\text{where }H\text{ uses }p. \)

\todo{Add cross-entropy image}
%\begin{center}
%  \includegraphics[width=0.5\linewidth]{img/cross-entropy.png}
%\end{center}

\Com The minimizer of the cross-entropy is \(q: =p\), due to the second formulation.

\Com Usually, \(q\) is the approximation of the unknown \(p\).

\Def[Kullback-Leibler Divergence]

For probability distributions \(p\) and \(q\) defined on the same probability space, the KL-divergence between \(p\) and \(q\) is defined as

\( KL(p,q) =  -\sum_{\vx\in\cX}p(\vx)\log\left(\frac{q(\vx)}{p(\vx)}\right) =  \sum_{\vx\in\cX}p(\vx)\log\left(\frac{p(\vx)}{q(\vx)}\right) \geq 0. \)

\( KL(p,q) =  -\Exp[\vx\sim p]{\log\left(\frac{q(\vx)}{p(\vx)}\right)} =  \Exp[\vx\sim p]{\log\left(\frac{p(\vx)}{q(\vx)}\right)} \geq 0. \)

\( KL(X;p,q) = H(p,q) - H(X), \quad\text{where }H\text{ uses }p.\)

The KL-divergence is defined only if \(\forall\vx\colon q(\vx) = 0\Longrightarrow p(\vx) = 0\) (absolute continuity). Whenever \(p(\vx)\) is zero the contribution of the corresponding term is interpreted as zero because

\( \lim_{x\to 0^+} x\log(x) = 0.\)

In ML it is a measure of the amount of information lost, when \(q\) (model) is used to approximate \(p\) (true).

\Com \(KL(p,y) = 0\Longleftrightarrow p\equiv q\).

\Com Note that the KL-divergence is not symmetric!

\sep

\todo{Theorem about entropy = minimizer of cross entropy.}

\sep

\Def[Jensen-Shannon Divergence]

\( JSD(P,Q) =  \frac{1}{2}KL(P,M) + \frac{1}{2}KL(Q,M)\in[0,\mcr{\log(n)?}] \) \quad \(M = \frac{1}{2}(P + Q)\)

\Cor The JSD is symmetric!

\Com The JSD is a symmetrized and smoothed version of the KL-divergence.
